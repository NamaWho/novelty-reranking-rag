{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc2f278c-b263-470d-bae7-4e06eb99b980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "864c71e6-404f-44f1-9805-4ce9ae976476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building passage store: 8841823it [00:24, 354442.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load MSMARCO passage train dataset\n",
    "dataset = ir_datasets.load('msmarco-passage/train')\n",
    "docs = {}\n",
    "for doc in tqdm(dataset.docs_iter(), desc=\"Building passage store\"):\n",
    "    docs[doc.doc_id] = doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c37e38-d130-4dce-8e9e-4abb7ac84f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the run file from Zenodo\n",
    "env_run_path = os.path.expanduser(\"zenodo/__colbert-10000-sampled-100__msmarco-passage-train-judged.run\")\n",
    "assert os.path.exists(env_run_path), f\"Run file not found at {env_run_path}\"\n",
    "run_lines = open(env_run_path, 'r').read().splitlines()\n",
    "# run_by_query: qid -> list of (docid, score, orig_cluster_label)\n",
    "run_by_query = defaultdict(list)\n",
    "for line in run_lines:\n",
    "    # format: qid orig_cluster docid rank score runname\n",
    "    qid, orig_cluster, docid, rank, score, _ = line.split()\n",
    "    run_by_query[qid].append((docid, int(float(score)), int(orig_cluster)))\n",
    "# Keep only top-100 per query\n",
    "for qid in run_by_query:\n",
    "    run_by_query[qid] = sorted(run_by_query[qid], key=lambda x: -x[1])[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02377ede-7a74-41b1-9f8b-abc6907afd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract original clusters (aligned with sorted docs)\n",
    "dev_clusters = {}\n",
    "test_clusters = {}\n",
    "orig_clusters = {qid: [c for (_, _, c) in docs_scores]\n",
    "                 for qid, docs_scores in run_by_query.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1b5c2b3-15ea-4683-9e99-70c04d0d28f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev queries: 2000, Test queries: 8000\n"
     ]
    }
   ],
   "source": [
    "# Split queries into dev (20%) and test (80%)\n",
    "random.seed(42)\n",
    "all_qids = list(run_by_query.keys())\n",
    "random.shuffle(all_qids)\n",
    "dev_size = int(0.2 * len(all_qids))\n",
    "dev_qids = all_qids[:dev_size]\n",
    "test_qids = all_qids[dev_size:]\n",
    "print(f\"Dev queries: {len(dev_qids)}, Test queries: {len(test_qids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1abf5c6-432a-49ce-8bbb-83b3080ba11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 3267.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from pyterrier_dr import RetroMAE, E5, BGEM3\n",
    "\n",
    "# Initialize models\n",
    "retro_mae_model = RetroMAE(device=device)\n",
    "e5_model = E5(device=device)\n",
    "bgem3_model = BGEM3(device=device)\n",
    "st_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "thresholds = [0.70, 0.72, 0.74, 0.76, 0.78, 0.80, 0.82, 0.85, 0.88, 0.90, 0.92]\n",
    "\n",
    "# Initialize all_clusters for all encoders, thresholds, and query IDs\n",
    "all_clusters = {encoder: {thr: {} for thr in thresholds} for encoder in ['retro_mae', 'e5', 'bgem3', 'st']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "990c7138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento degli embeddings esistenti...\n",
      "Embeddings caricati da: rq1/doc_embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "# Percorso per salvare gli embeddings\n",
    "embeddings_path = \"rq1/doc_embeddings.pkl\"\n",
    "\n",
    "# Funzione per salvare gli embeddings\n",
    "def save_embeddings(embeddings, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "    print(f\"Embeddings salvati in: {path}\")\n",
    "\n",
    "# Funzione per caricare gli embeddings\n",
    "def load_embeddings(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    print(f\"Embeddings caricati da: {path}\")\n",
    "    return embeddings\n",
    "\n",
    "# Funzione per calcolare gli embeddings per una query\n",
    "def process_query(qid, docs_scores, docs, retro_mae_model, e5_model, bgem3_model, st_model):\n",
    "    doc_ids = [docid for docid, _, _ in docs_scores]\n",
    "    doc_texts = [docs[docid] for docid in doc_ids]\n",
    "\n",
    "    # Calcola gli embeddings\n",
    "    retro_mae_embeds = retro_mae_model.encode_docs(doc_texts)\n",
    "    e5_embeds = e5_model.encode_docs(doc_texts)\n",
    "    bgem3_embeds = bgem3_model.encode_docs(doc_texts)\n",
    "    st_embeds = st_model.encode(doc_texts)\n",
    "\n",
    "    return qid, {\n",
    "        \"retro_mae\": retro_mae_embeds,\n",
    "        \"e5\": e5_embeds,\n",
    "        \"bgem3\": bgem3_embeds,\n",
    "        \"st\": st_embeds\n",
    "    }\n",
    "\n",
    "# Controlla se gli embeddings esistono\n",
    "if os.path.exists(embeddings_path):\n",
    "    print(\"Caricamento degli embeddings esistenti...\")\n",
    "    doc_embeddings = load_embeddings(embeddings_path)\n",
    "else:\n",
    "    print(\"Calcolo degli embeddings...\")\n",
    "\n",
    "    # Organizza i risultati in un dizionario\n",
    "    doc_embeddings = {\n",
    "        \"retro_mae\": {},\n",
    "        \"e5\": {},\n",
    "        \"bgem3\": {},\n",
    "        \"st\": {}\n",
    "    }\n",
    "\n",
    "    for qid, docs_scores in tqdm(run_by_query.items(), desc=\"Processing queries\"):\n",
    "        qid, embeddings = process_query(qid, docs_scores, docs, retro_mae_model, e5_model, bgem3_model, st_model)\n",
    "        for model_name in doc_embeddings:\n",
    "            if qid not in doc_embeddings[model_name]:\n",
    "                doc_embeddings[model_name][qid] = {}\n",
    "            doc_ids = [docid for docid, _, _ in docs_scores]\n",
    "            for docid, emb in zip(doc_ids, embeddings[model_name]):\n",
    "                doc_embeddings[model_name][qid][docid] = emb\n",
    "\n",
    "    save_embeddings(doc_embeddings, embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7261d6c8-d4dc-4a15-9c05-65a49fe93798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering with retro_mae embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering queries with encoder retro_mae: 100%|██████████| 10000/10000 [03:23<00:00, 49.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering with e5 embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering queries with encoder e5: 100%|██████████| 10000/10000 [03:20<00:00, 49.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering with bgem3 embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering queries with encoder bgem3: 100%|██████████| 10000/10000 [04:19<00:00, 38.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering with st embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering queries with encoder st: 100%|██████████| 10000/10000 [02:04<00:00, 80.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Perform clustering and collect cluster assignments per threshold\n",
    "for encoder, qid_dict in doc_embeddings.items():\n",
    "    print(f\"Clustering with {encoder} embeddings...\")\n",
    "    for qid, docs_scores in tqdm(list(run_by_query.items()), desc=f\"Clustering queries with encoder {encoder}\"):\n",
    "        doc_ids = [d for d, _, _ in docs_scores]\n",
    "        assert all(docid in qid_dict[qid] for docid in doc_ids), f\"Missing embedding in encoder={encoder}, qid={qid}\"\n",
    "\n",
    "        # Ottieni l'embedding di ciascun docid per la query qid\n",
    "        embeddings = np.array([\n",
    "            qid_dict[qid][docid] for docid in doc_ids if docid in qid_dict[qid]\n",
    "        ])\n",
    "\n",
    "        for thr in thresholds:\n",
    "            clustering = AgglomerativeClustering(\n",
    "                n_clusters=None,\n",
    "                metric='cosine',\n",
    "                linkage='complete',\n",
    "                distance_threshold=1 - thr\n",
    "            )\n",
    "            labels = clustering.fit_predict(embeddings)\n",
    "            all_clusters[encoder][thr][qid] = labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82621c4f-ca87-41dd-abb1-eb3a313078b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting best threshold for encoder: retro_mae\n",
      "Best threshold for retro_mae: 0.9 (Silhouette: 0.2583, Calinski-Harabasz: 10.23)\n",
      "Selecting best threshold for encoder: e5\n",
      "Best threshold for e5: 0.92 (Silhouette: 0.2919, Calinski-Harabasz: 7.10)\n",
      "Selecting best threshold for encoder: bgem3\n",
      "Best threshold for bgem3: 0.88 (Silhouette: 0.2854, Calinski-Harabasz: 10.04)\n",
      "Selecting best threshold for encoder: st\n",
      "Best threshold for st: 0.85 (Silhouette: 0.2916, Calinski-Harabasz: 11.43)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoder</th>\n",
       "      <th>threshold</th>\n",
       "      <th>silhouette_avg</th>\n",
       "      <th>calinski_harabasz_avg</th>\n",
       "      <th>combined_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>retro_mae</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.233340</td>\n",
       "      <td>4.950234</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>retro_mae</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.239947</td>\n",
       "      <td>4.997737</td>\n",
       "      <td>0.189064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>retro_mae</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.245810</td>\n",
       "      <td>5.092865</td>\n",
       "      <td>0.363822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>retro_mae</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.251888</td>\n",
       "      <td>5.223333</td>\n",
       "      <td>0.549200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>retro_mae</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.257979</td>\n",
       "      <td>5.417000</td>\n",
       "      <td>0.743263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>retro_mae</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.263091</td>\n",
       "      <td>5.708427</td>\n",
       "      <td>0.923135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>retro_mae</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.266862</td>\n",
       "      <td>6.127910</td>\n",
       "      <td>1.082806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>retro_mae</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.269484</td>\n",
       "      <td>7.062135</td>\n",
       "      <td>1.278571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>retro_mae</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.265850</td>\n",
       "      <td>8.634892</td>\n",
       "      <td>1.385479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>retro_mae</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.258348</td>\n",
       "      <td>10.227179</td>\n",
       "      <td>1.387956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>retro_mae</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.243684</td>\n",
       "      <td>12.531436</td>\n",
       "      <td>1.286198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>e5</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.189833</td>\n",
       "      <td>6.978940</td>\n",
       "      <td>0.949025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>e5</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.196380</td>\n",
       "      <td>6.511047</td>\n",
       "      <td>0.812060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>e5</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.201739</td>\n",
       "      <td>6.061051</td>\n",
       "      <td>0.671164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>e5</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.205867</td>\n",
       "      <td>5.656856</td>\n",
       "      <td>0.537887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>e5</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.213355</td>\n",
       "      <td>5.359148</td>\n",
       "      <td>0.483272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>e5</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.217660</td>\n",
       "      <td>5.020043</td>\n",
       "      <td>0.379709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>e5</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.228338</td>\n",
       "      <td>4.822897</td>\n",
       "      <td>0.399557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>e5</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.244689</td>\n",
       "      <td>4.770625</td>\n",
       "      <td>0.537222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>e5</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.266887</td>\n",
       "      <td>5.121375</td>\n",
       "      <td>0.905341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>e5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.282569</td>\n",
       "      <td>5.772456</td>\n",
       "      <td>1.338722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>e5</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.291944</td>\n",
       "      <td>7.097556</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>bgem3</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.252338</td>\n",
       "      <td>5.138374</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bgem3</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.256704</td>\n",
       "      <td>5.233008</td>\n",
       "      <td>0.134028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bgem3</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.262230</td>\n",
       "      <td>5.400138</td>\n",
       "      <td>0.308781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bgem3</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.267241</td>\n",
       "      <td>5.601322</td>\n",
       "      <td>0.472599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>bgem3</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.273438</td>\n",
       "      <td>5.915830</td>\n",
       "      <td>0.682267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>bgem3</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.279057</td>\n",
       "      <td>6.331682</td>\n",
       "      <td>0.886450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>bgem3</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.283848</td>\n",
       "      <td>6.913700</td>\n",
       "      <td>1.085096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>bgem3</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.287596</td>\n",
       "      <td>8.143133</td>\n",
       "      <td>1.323949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>bgem3</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.285419</td>\n",
       "      <td>10.040545</td>\n",
       "      <td>1.466771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>bgem3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.277553</td>\n",
       "      <td>11.874668</td>\n",
       "      <td>1.441398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>bgem3</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.263322</td>\n",
       "      <td>14.413779</td>\n",
       "      <td>1.311541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>st</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.291706</td>\n",
       "      <td>6.798749</td>\n",
       "      <td>0.857423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>st</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.293428</td>\n",
       "      <td>7.046029</td>\n",
       "      <td>0.913627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>st</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.295376</td>\n",
       "      <td>7.362129</td>\n",
       "      <td>0.979910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>st</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.297157</td>\n",
       "      <td>7.745911</td>\n",
       "      <td>1.047519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>st</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.298184</td>\n",
       "      <td>8.233787</td>\n",
       "      <td>1.106257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>st</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.297500</td>\n",
       "      <td>8.877818</td>\n",
       "      <td>1.138892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>st</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.296611</td>\n",
       "      <td>9.677399</td>\n",
       "      <td>1.178530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>st</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.291631</td>\n",
       "      <td>11.428146</td>\n",
       "      <td>1.198554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>st</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.280951</td>\n",
       "      <td>14.027178</td>\n",
       "      <td>1.155953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>st</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.269394</td>\n",
       "      <td>16.570816</td>\n",
       "      <td>1.089928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>st</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.252748</td>\n",
       "      <td>20.304110</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      encoder  threshold  silhouette_avg  calinski_harabasz_avg  \\\n",
       "0   retro_mae       0.70        0.233340               4.950234   \n",
       "1   retro_mae       0.72        0.239947               4.997737   \n",
       "2   retro_mae       0.74        0.245810               5.092865   \n",
       "3   retro_mae       0.76        0.251888               5.223333   \n",
       "4   retro_mae       0.78        0.257979               5.417000   \n",
       "5   retro_mae       0.80        0.263091               5.708427   \n",
       "6   retro_mae       0.82        0.266862               6.127910   \n",
       "7   retro_mae       0.85        0.269484               7.062135   \n",
       "8   retro_mae       0.88        0.265850               8.634892   \n",
       "9   retro_mae       0.90        0.258348              10.227179   \n",
       "10  retro_mae       0.92        0.243684              12.531436   \n",
       "11         e5       0.70        0.189833               6.978940   \n",
       "12         e5       0.72        0.196380               6.511047   \n",
       "13         e5       0.74        0.201739               6.061051   \n",
       "14         e5       0.76        0.205867               5.656856   \n",
       "15         e5       0.78        0.213355               5.359148   \n",
       "16         e5       0.80        0.217660               5.020043   \n",
       "17         e5       0.82        0.228338               4.822897   \n",
       "18         e5       0.85        0.244689               4.770625   \n",
       "19         e5       0.88        0.266887               5.121375   \n",
       "20         e5       0.90        0.282569               5.772456   \n",
       "21         e5       0.92        0.291944               7.097556   \n",
       "22      bgem3       0.70        0.252338               5.138374   \n",
       "23      bgem3       0.72        0.256704               5.233008   \n",
       "24      bgem3       0.74        0.262230               5.400138   \n",
       "25      bgem3       0.76        0.267241               5.601322   \n",
       "26      bgem3       0.78        0.273438               5.915830   \n",
       "27      bgem3       0.80        0.279057               6.331682   \n",
       "28      bgem3       0.82        0.283848               6.913700   \n",
       "29      bgem3       0.85        0.287596               8.143133   \n",
       "30      bgem3       0.88        0.285419              10.040545   \n",
       "31      bgem3       0.90        0.277553              11.874668   \n",
       "32      bgem3       0.92        0.263322              14.413779   \n",
       "33         st       0.70        0.291706               6.798749   \n",
       "34         st       0.72        0.293428               7.046029   \n",
       "35         st       0.74        0.295376               7.362129   \n",
       "36         st       0.76        0.297157               7.745911   \n",
       "37         st       0.78        0.298184               8.233787   \n",
       "38         st       0.80        0.297500               8.877818   \n",
       "39         st       0.82        0.296611               9.677399   \n",
       "40         st       0.85        0.291631              11.428146   \n",
       "41         st       0.88        0.280951              14.027178   \n",
       "42         st       0.90        0.269394              16.570816   \n",
       "43         st       0.92        0.252748              20.304110   \n",
       "\n",
       "    combined_score  \n",
       "0         0.000000  \n",
       "1         0.189064  \n",
       "2         0.363822  \n",
       "3         0.549200  \n",
       "4         0.743263  \n",
       "5         0.923135  \n",
       "6         1.082806  \n",
       "7         1.278571  \n",
       "8         1.385479  \n",
       "9         1.387956  \n",
       "10        1.286198  \n",
       "11        0.949025  \n",
       "12        0.812060  \n",
       "13        0.671164  \n",
       "14        0.537887  \n",
       "15        0.483272  \n",
       "16        0.379709  \n",
       "17        0.399557  \n",
       "18        0.537222  \n",
       "19        0.905341  \n",
       "20        1.338722  \n",
       "21        2.000000  \n",
       "22        0.000000  \n",
       "23        0.134028  \n",
       "24        0.308781  \n",
       "25        0.472599  \n",
       "26        0.682267  \n",
       "27        0.886450  \n",
       "28        1.085096  \n",
       "29        1.323949  \n",
       "30        1.466771  \n",
       "31        1.441398  \n",
       "32        1.311541  \n",
       "33        0.857423  \n",
       "34        0.913627  \n",
       "35        0.979910  \n",
       "36        1.047519  \n",
       "37        1.106257  \n",
       "38        1.138892  \n",
       "39        1.178530  \n",
       "40        1.198554  \n",
       "41        1.155953  \n",
       "42        1.089928  \n",
       "43        1.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "# Select best threshold on dev set using internal metrics (silhouette)\n",
    "best_thresholds = {}\n",
    "all_results = []\n",
    "\n",
    "for encoder, clusters_by_threshold in all_clusters.items():\n",
    "    print(f\"Selecting best threshold for encoder: {encoder}\")\n",
    "    sil_avgs = {}\n",
    "    ch_avgs = {}\n",
    "    for thr in thresholds:\n",
    "        sil_scores = []\n",
    "        ch_scores = []\n",
    "        for qid in dev_qids:\n",
    "            labels = np.array(clusters_by_threshold[thr][qid])\n",
    "            if len(set(labels)) > 1:\n",
    "                doc_ids = [d for d, _, _ in run_by_query[qid]]\n",
    "                \n",
    "                emb = np.array([\n",
    "                    doc_embeddings[encoder][qid][docid]\n",
    "                    for docid in doc_ids\n",
    "                    if docid in doc_embeddings[encoder][qid]\n",
    "                ])\n",
    "                \n",
    "                # Calcola lo score solo se hai abbastanza embedding\n",
    "                if len(emb) == len(labels):\n",
    "                    sil_scores.append(silhouette_score(emb, labels, metric='cosine'))\n",
    "                    ch_scores.append(calinski_harabasz_score(emb, labels))\n",
    "        sil_avgs[thr] = np.mean(sil_scores) if sil_scores else -1.0\n",
    "        ch_avgs[thr] = np.mean(ch_scores) if ch_scores else -1.0\n",
    "    \n",
    "    # Normalize scores\n",
    "    sil_norm = minmax_scale([sil_avgs[thr] for thr in thresholds])\n",
    "    ch_norm = minmax_scale([ch_avgs[thr] for thr in thresholds])\n",
    "    combined_scores = sil_norm + ch_norm\n",
    "\n",
    "\n",
    "    for i, thr in enumerate(thresholds):\n",
    "        all_results.append({\n",
    "            'encoder': encoder,\n",
    "            'threshold': thr,\n",
    "            'silhouette_avg': sil_avgs[thr],\n",
    "            'calinski_harabasz_avg': ch_avgs[thr],\n",
    "            'combined_score': combined_scores[i]\n",
    "        })\n",
    "\n",
    "    best_thr = thresholds[np.argmax(combined_scores)]\n",
    "    best_thresholds[encoder] = best_thr\n",
    "\n",
    "    print(f\"Best threshold for {encoder}: {best_thr} \"\n",
    "        f\"(Silhouette: {sil_avgs[best_thr]:.4f}, \"\n",
    "        f\"Calinski-Harabasz: {ch_avgs[best_thr]:.2f})\")\n",
    "    \n",
    "df_all = pd.DataFrame(all_results)\n",
    "df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb03331b-167b-49ee-ac7f-5fdd3ad0047b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoder</th>\n",
       "      <th>threshold</th>\n",
       "      <th>avg_silhouette</th>\n",
       "      <th>avg_calinski_harabasz</th>\n",
       "      <th>combined_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>retro_mae</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.255388</td>\n",
       "      <td>10.213800</td>\n",
       "      <td>0.731408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e5</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.291616</td>\n",
       "      <td>7.068144</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bgem3</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.284157</td>\n",
       "      <td>10.030566</td>\n",
       "      <td>1.482904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>st</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.289375</td>\n",
       "      <td>11.368969</td>\n",
       "      <td>1.938125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     encoder  threshold  avg_silhouette  avg_calinski_harabasz  combined_score\n",
       "0  retro_mae       0.90        0.255388              10.213800        0.731408\n",
       "1         e5       0.92        0.291616               7.068144        1.000000\n",
       "2      bgem3       0.88        0.284157              10.030566        1.482904\n",
       "3         st       0.85        0.289375              11.368969        1.938125"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate selected threshold on test set (internal metrics)\n",
    "results = []\n",
    "\n",
    "for encoder, best_thr in best_thresholds.items():\n",
    "    labels_test = all_clusters[encoder][best_thr]\n",
    "    sil_scores, ch_scores = [], []\n",
    "    for qid in test_qids:\n",
    "        labels = np.array(labels_test[qid])\n",
    "        if len(set(labels)) > 1:\n",
    "            doc_ids = [d for d, _, _ in run_by_query[qid]]\n",
    "\n",
    "            emb = np.array([\n",
    "                doc_embeddings[encoder][qid][docid]\n",
    "                for docid in doc_ids\n",
    "                if docid in doc_embeddings[encoder][qid]\n",
    "            ])\n",
    "            \n",
    "            if len(emb) == len(labels):  # Verifica che il numero di embedding corrisponda alle label\n",
    "                sil_scores.append(silhouette_score(emb, labels, metric='cosine'))\n",
    "                ch_scores.append(calinski_harabasz_score(emb, labels))\n",
    "    \n",
    "    avg_silhouette = np.mean(sil_scores) if sil_scores else -1.0\n",
    "    avg_calinski_harabasz = np.mean(ch_scores) if ch_scores else -1.0\n",
    "    results.append({\n",
    "        'encoder': encoder, \n",
    "        'threshold': best_thr, \n",
    "        'avg_silhouette': avg_silhouette,\n",
    "        'avg_calinski_harabasz': avg_calinski_harabasz\n",
    "        })\n",
    "    \n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "if len(df_results) > 1:\n",
    "    sil_norm = minmax_scale(df_results['avg_silhouette'])\n",
    "    ch_norm = minmax_scale(df_results['avg_calinski_harabasz'])\n",
    "    df_results['combined_score'] = sil_norm + ch_norm\n",
    "else:\n",
    "    df_results['combined_score'] = df_results['avg_silhouette'] + df_results['avg_calinski_harabasz']\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ca41b9c-790a-411d-aa1c-dd77f80e8156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving run file for encoder retro_mae with threshold 0.9\n",
      "Saved test run file: rq1/run_retro_mae_thr_90_test.run\n",
      "Saving run file for encoder e5 with threshold 0.92\n",
      "Saved test run file: rq1/run_e5_thr_92_test.run\n",
      "Saving run file for encoder bgem3 with threshold 0.88\n",
      "Saved test run file: rq1/run_bgem3_thr_88_test.run\n",
      "Saving run file for encoder st with threshold 0.85\n",
      "Saved test run file: rq1/run_st_thr_85_test.run\n",
      "Run files saved for all encoders with best thresholds.\n"
     ]
    }
   ],
   "source": [
    "# 9. Save run files for test set only: original and best threshold\n",
    "os.makedirs(\"rq1\", exist_ok=True)\n",
    "\n",
    "for encoder, best_thr in best_thresholds.items():\n",
    "    print(f\"Saving run file for encoder {encoder} with threshold {best_thr}\")\n",
    "    labels_test = all_clusters[encoder][best_thr]\n",
    "    tag = f\"run_{encoder}_thr_{int(best_thr*100)}\"\n",
    "    run_fname = f\"rq1/{tag}_test.run\"\n",
    "    with open(run_fname, 'w') as fout:\n",
    "        for qid in test_qids:\n",
    "            docs_scores = run_by_query[qid]\n",
    "            labels = labels_test[qid]\n",
    "            for rank_idx, (docid, score, orig_c) in enumerate(docs_scores, start=1):\n",
    "                fout.write(f\"{qid} {labels[rank_idx-1]} {docid} {rank_idx} {score} {tag}\\n\")\n",
    "    print(f\"Saved test run file: {run_fname}\")\n",
    "\n",
    "print(\"Run files saved for all encoders with best thresholds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1404ac4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dceb484f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/emalir/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/opt/miniconda3/envs/emalir/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n",
      "/opt/miniconda3/envs/emalir/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/opt/miniconda3/envs/emalir/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n",
      "/opt/miniconda3/envs/emalir/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/opt/miniconda3/envs/emalir/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n",
      "/opt/miniconda3/envs/emalir/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/opt/miniconda3/envs/emalir/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'browser'\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import umap.umap_ as umap\n",
    "\n",
    "qid = dev_qids[0]  # Scegli una query da visualizzare\n",
    "encoders = ['retro_mae', 'e5', 'bgem3', 'st']\n",
    "fig, axes = plt.subplots(1, len(encoders), figsize=(6*len(encoders), 6), sharex=False, sharey=False)\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for idx, encoder in enumerate(['retro_mae', 'e5', 'bgem3', 'st']):\n",
    "    thr = best_thresholds[encoder]  # Scegli il threshold migliore per l'encoder\n",
    "    doc_ids = [d for d, _, _ in run_by_query[qid]]\n",
    "    embeddings = np.array([\n",
    "        doc_embeddings[encoder][qid][docid] for docid in doc_ids if docid in doc_embeddings[encoder][qid]\n",
    "    ])\n",
    "    labels = np.array(all_clusters[encoder][thr][qid])\n",
    "    labels = label_encoder.fit_transform(labels)  # Codifica le etichette numeriche\n",
    "    texts = [docs[docid] for docid in doc_ids if docid in doc_embeddings[encoder][qid]]\n",
    "\n",
    "    # Riduzione con UMAP\n",
    "    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=3, random_state=42)\n",
    "    embedding_3d = reducer.fit_transform(embeddings)\n",
    "\n",
    "    # Costruzione DataFrame\n",
    "    df_plot = pd.DataFrame({\n",
    "        'x': embedding_3d[:, 0],\n",
    "        'y': embedding_3d[:, 1],\n",
    "        'z': embedding_3d[:, 2],\n",
    "        'cluster': labels,\n",
    "        'doc_id': doc_ids[:len(embedding_3d)],\n",
    "        'text': texts[:len(embedding_3d)]\n",
    "    })\n",
    "\n",
    "    # Plotly interactive scatter plot\n",
    "    fig = px.scatter_3d(\n",
    "    df_plot, x='x', y='y', z='z',\n",
    "    color=df_plot['cluster'].astype(str),\n",
    "    hover_data=['doc_id', 'text'],\n",
    "    title=f'Visualizzazione 3D dei Cluster - Query {qid} - Encoder {encoder}',\n",
    "    color_discrete_sequence=px.colors.qualitative.Bold\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=5))\n",
    "    fig.show()\n",
    "#     # Visualizzazione\n",
    "#     ax = axes[idx]\n",
    "#     scatter = ax.scatter(embedding_2d[:, 0], embedding_2d[:, 1], c=labels, cmap='hsv', s=50)\n",
    "#     ax.set_title(f'Encoder: {encoder}')\n",
    "#     ax.set_xlabel('Dim 1')\n",
    "#     ax.set_ylabel('Dim 2')\n",
    "#     fig.colorbar(scatter, ax=ax, label='Cluster')\n",
    "\n",
    "# plt.suptitle(f'Visualizzazione dei Cluster per la Query {qid}', fontsize=16)\n",
    "# plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emalir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
