{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cefb03b-ca0a-485c-a93e-ad12bf4a1387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyterrier as pt\n",
    "import pyterrier_rag as ptr\n",
    "from datasets import load_dataset\n",
    "import ir_datasets\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b9b8566-7a7a-42ee-b1fb-1029d3f53d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Carico MMLU da Hugging Face...\n",
      "üîç Carico ranking esistente...\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 2. Caricamento dataset MMLU\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"üîç Carico MMLU da Hugging Face...\")\n",
    "mmlu = load_dataset(\"cais/mmlu\", \"all\", split=\"test\")  # HF loader :contentReference[oaicite:7]{index=7}\n",
    "# Rinomina e prepara DataFrame\n",
    "#df_mmlu = pd.DataFrame(mmlu) \\\n",
    "#    .rename(columns={\"question\": \"query\", \"answer\": \"gold_answer\"}) \\\n",
    "#    .assign(qid=lambda df: df.index.astype(str))\n",
    "\n",
    "\n",
    "# Costruisci il DataFrame e imposta il nuovo qid = subject + \"_\" + index\n",
    "df_mmlu = (\n",
    "    pd.DataFrame(mmlu)\n",
    "      # crea qid unendo subject e index\n",
    "      .assign(qid=lambda df: df[\"subject\"] + \"_\" + df.index.astype(str))\n",
    "      # rinomina le colonne per lo script RAG\n",
    "      .rename(columns={\n",
    "          \"question\": \"query\",\n",
    "          \"answer\": \"gold_answer\"\n",
    "      })\n",
    ")\n",
    "\n",
    "print(\"üîç Carico ranking esistente...\")\n",
    "df_run_base = pd.read_csv(\n",
    "    \"../data/processed/rag/__setencoder-novelty-base__msmarco-segment-mmlu.tsv\", sep=\"\\t\",\n",
    "    names=[\"qid\", \"Q0\", \"doc_id\", \"rank\", \"score\", \"run_name\", \"text\"]\n",
    ")\n",
    "df_run_ea = pd.read_csv(\n",
    "    \"../data/processed/rag/__setencoder-novelty-ea__msmarco-segment-mmlu.tsv\", sep=\"\\t\",\n",
    "    names=[\"qid\", \"Q0\", \"doc_id\", \"rank\", \"score\", \"run_name\", \"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f097ec-5fab-4db1-aa00-c3b8b803adaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 897996/113520750 [00:33<1:00:15, 31147.07it/s]"
     ]
    }
   ],
   "source": [
    "dataset = ir_datasets.load('msmarco-segment-v2.1')\n",
    "pt_dataset = pt.get_dataset(\"irds:msmarco-segment-v2.1\")\n",
    "total_docs = dataset.docs_count() \n",
    "all_docs = list(tqdm(dataset.docs_iter(), total=total_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5539f99c-d015-4e93-ba48-e45392827e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict = {doc.doc_id: doc for doc in all_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad24324e-d41a-4dd5-8330-76ded4e6d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier_rag.backend import OpenAIBackend\n",
    "from pyterrier_rag.prompt import Concatenator\n",
    "from pyterrier_rag.readers import Reader\n",
    "from pyterrier_rag.prompt import PromptTransformer, prompt\n",
    "from fastchat.model import get_conversation_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1857ec0-22cb-4085-9f6e-eb74c21ec5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"llama-3-8b-instruct\"\n",
    "model_name = \"llama-3.3-70b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4aad4c-346c-40af-a0c3-4a4a0b076073",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = r\"\"\"You are an expert Q&A system that is trusted around the world. \n",
    "        Always answer the query using the provided context information,\n",
    "        and not prior knowledge.\n",
    "\n",
    "        Some rules to follow:\n",
    "        1. Never directly reference the given context in your answer\n",
    "        2. Avoid statements like 'Based on the context, ...' or \n",
    "        'The context information ...' or anything along those lines.\n",
    "        3. Output must be a single uppercase letter: A, B, C, or D ‚Äî nothing else.\"\"\"\n",
    "prompt_text = \"\"\"Context information is below.\n",
    "            ---------------------\n",
    "            {{ qcontext }}\n",
    "            ---------------------\n",
    "            Given the context information and a multiple-choice question, choose the correct answer.\n",
    "            \n",
    "            Query: {{ query }}\n",
    "\n",
    "            Choices: {{ choices }}\n",
    "\n",
    "            Answer with only the letter [\"A\", \"B\", \"C\", \"D\"] corresponding to the correct choice, with no mention of \"\". Do not include any explanation or additional text.\n",
    "\n",
    "            Answer: \"\"\"\n",
    "\n",
    "template = get_conversation_template(\"meta-llama-3.1-sp\")\n",
    "prompt = PromptTransformer(\n",
    "    conversation_template=template,\n",
    "    system_message=system_message,\n",
    "    instruction=prompt_text,\n",
    "    input_fields=[\"query\", \"qcontext\", 'choices'],\n",
    "    api_type=\"openai\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8cfc4c-c402-4586-949f-7e3d56e5f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank(df_queries):\n",
    "    run = df_queries.merge(df_run_base, on=\"qid\", how=\"left\")\n",
    "    return run\n",
    "get_rank_pipe = pt.apply.generic(get_rank)\n",
    "\n",
    "def get_rank_common_docs(df_queries, k=5):\n",
    "    rows = []\n",
    "\n",
    "    for _, row in df_queries.iterrows():\n",
    "        qid = row[\"qid\"]\n",
    "        docs_base = set(df_run_base[df_run_base[\"qid\"] == qid].sort_values(\"rank\").head(k)[\"doc_id\"])\n",
    "        docs_ea = set(df_run_ea[df_run_ea[\"qid\"] == qid].sort_values(\"rank\").head(k)[\"doc_id\"])\n",
    "\n",
    "        common_docs = docs_base & docs_ea\n",
    "\n",
    "        if len(common_docs) == 0:\n",
    "            final_docs = list(docs_ea)\n",
    "        else:\n",
    "            n_missing = k - len(common_docs)\n",
    "    \n",
    "            random_docs = []\n",
    "            while len(random_docs) < n_missing:\n",
    "                random_doc = all_docs[np.random.randint(0, len(all_docs))]\n",
    "                if random_doc.doc_id not in docs_base and random_doc.doc_id not in docs_ea:\n",
    "                    print(random_doc.doc_id)\n",
    "                    random_docs.append(random_doc.doc_id)\n",
    "    \n",
    "            final_docs = list(common_docs) + random_docs\n",
    "        \n",
    "        for rank, doc_id in enumerate(final_docs):\n",
    "            rows.append({\n",
    "                \"query\": row['query'],\n",
    "                \"qid\": qid,\n",
    "                \"doc_id\": doc_id,\n",
    "                \"rank\": rank,\n",
    "                \"score\": 1.0 - 0.01 * rank,  # dummy score\n",
    "                \"text\": doc_dict.get(doc_id).segment, \n",
    "                \"choices\": row['choices']\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "get_rank_commondocs_pipe = pt.apply.generic(get_rank_common_docs)\n",
    "\n",
    "def get_top1_plus_random(df_queries, k=3):\n",
    "    rows = []\n",
    "\n",
    "    for _, row in df_queries.iterrows():\n",
    "        qid = row[\"qid\"]\n",
    "\n",
    "        # 1. Recupera il primo documento per la query\n",
    "        top_doc = df_run_ea[df_run_ea[\"qid\"] == qid].sort_values(\"rank\").iloc[0]\n",
    "        final_docs = [top_doc.doc_id]\n",
    "\n",
    "        # 2. Aggiungi documenti random da all_docs (evitando duplicati)\n",
    "        n_needed = k - 1\n",
    "        added = 0\n",
    "        random_docs = []\n",
    "\n",
    "        while added < n_needed:\n",
    "            random_doc = all_docs[np.random.randint(0, len(all_docs))]\n",
    "            if random_doc.doc_id not in final_docs:\n",
    "                random_docs.append(random_doc.doc_id)\n",
    "                final_docs.append(random_doc.doc_id)\n",
    "                added += 1\n",
    "\n",
    "        # 3. Costruisci le righe per questa query\n",
    "        for rank, doc_id in enumerate(final_docs):\n",
    "            rows.append({\n",
    "                \"query\": row[\"query\"],\n",
    "                \"qid\": qid,\n",
    "                \"doc_id\": doc_id,\n",
    "                \"rank\": rank,\n",
    "                \"score\": 1.0 - 0.01 * rank,  # dummy score decrescente\n",
    "                \"text\": doc_dict.get(doc_id).segment,\n",
    "                \"choices\": row[\"choices\"]\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "get_rank_top1_plus_random_pipe = pt.apply.generic(get_top1_plus_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78513c1f-8861-459e-bb9c-5c96f1fb78fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"casperhansen/llama-3-8b-instruct-awq\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"casperhansen/llama-3.3-70b-instruct-awq\")\n",
    "\n",
    "generation_args={\n",
    "    \"temperature\": 0.01,\n",
    "    \"max_tokens\": 1,\n",
    "}\n",
    "\n",
    "# this could equally be a real OpenAI models\n",
    "llama = OpenAIBackend(model_name, \n",
    "                      api_key=os.environ['IDA_LLM_API_KEY'],\n",
    "                      generation_args=generation_args,\n",
    "                      base_url=\"http://api.llm.apps.os.dcs.gla.ac.uk/v1\", \n",
    "                      verbose=True, \n",
    "                      parallel=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6058c64-f79a-46f5-8f12-08dcec909ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_reader = Reader(llama, prompt=prompt)\n",
    "#set_encoder_llama = get_rank_pipe % 3 >> Concatenator(tokenizer=tokenizer, max_length=8191,max_per_context=819,additional_fields=[\"choices\"]) >> llama_reader\n",
    "#set_encoder_llama = get_rank_commondocs_pipe % 5 >> Concatenator(tokenizer=tokenizer, max_length=8191,max_per_context=2730,additional_fields=[\"choices\"]) >> llama_reader\n",
    "set_encoder_llama = get_rank_top1_plus_random_pipe % 3 >> Concatenator(tokenizer=tokenizer, max_length=8191,max_per_context=2730,additional_fields=[\"choices\"]) >> llama_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba5dd0-dec5-4844-b2a7-428f2f4489f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"‚öôÔ∏è Esecuzione pipeline RAG su MMLU‚Ä¶\")\n",
    "#results = set_encoder_llama.transform(df_mmlu.head(1000))\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "save_every=100\n",
    "partial_save_path=\"../data/processed/rag/ea_top1plusrandom_mmlu_rag_output_cut_3.tsv\"\n",
    "\n",
    "try:\n",
    "    df_partial = pd.read_csv(partial_save_path, sep=\"\\t\")\n",
    "    done_qids = set(df_partial[\"qid\"])\n",
    "    print(f\"‚úÖ Ripresi {len(done_qids)} risultati da salvataggio parziale.\")\n",
    "    results = [df_partial]\n",
    "except FileNotFoundError:\n",
    "    print(\"üö® Nessun salvataggio trovato, si parte da zero.\")\n",
    "    done_qids = set()\n",
    "    results = []\n",
    "\n",
    "remaining = df_mmlu[~df_mmlu[\"qid\"].isin(done_qids)].reset_index(drop=True)\n",
    "print(f\"üß† Da processare: {len(remaining)} esempi.\")\n",
    "    \n",
    "for row in tqdm(remaining.iterrows(), total=len(remaining), desc=\"üîÅ RAG on MMLU\"):\n",
    "    idx, data = row\n",
    "    result = set_encoder_llama.transform(pd.DataFrame([data]))\n",
    "\n",
    "    result_merged = result.merge(df_mmlu[[\"qid\", \"gold_answer\"]], on=\"qid\", how=\"left\")\n",
    "    results.append(result_merged)\n",
    "\n",
    "    # ‚úÖ Salvataggio intermedio\n",
    "    if (idx + 1) % save_every == 0 or (idx + 1) == len(remaining):\n",
    "        df_save = pd.concat(results, ignore_index=True)\n",
    "        df_save.to_csv(partial_save_path, sep=\"\\t\", index=False)\n",
    "        print(f\"üíæ Salvati {len(df_save)} risultati su {partial_save_path}\")\n",
    "        results = [df_save]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa370b94-c10b-4888-b953-26be27c8749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 6. Valutazione\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "if isinstance(results, list):\n",
    "    results = pd.concat(results, ignore_index=True)\n",
    "    \n",
    "# üîÅ Converte i valori numerici in lettere\n",
    "index_to_choice = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
    "results[\"gold_answer\"] = results[\"gold_answer\"].map(index_to_choice)\n",
    "\n",
    "def evaluate(preds, golds):\n",
    "    preds = [str(p).strip().lower() for p in preds]\n",
    "    golds = [str(g).strip().lower() for g in golds]\n",
    "    return accuracy_score(golds, preds)\n",
    "\n",
    "acc = evaluate(results[\"qanswer\"], results[\"gold_answer\"])\n",
    "print(f\"\\nüìä MMLU Accuracy (EM): {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b47a8d3-6d83-4b4b-9b51-2fa79cd4bdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrai il subject da qid\n",
    "results[\"subject\"] = results[\"qid\"].apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n",
    "\n",
    "# Calcola accuracy per subject e raccoglila in una lista di dizionari\n",
    "subject_stats = []\n",
    "\n",
    "for subject in sorted(results[\"subject\"].unique()):\n",
    "    sub_df = results[results[\"subject\"] == subject]\n",
    "    acc_sub = evaluate(sub_df[\"qanswer\"], sub_df[\"gold_answer\"])\n",
    "    subject_stats.append({\n",
    "        \"subject\": subject,\n",
    "        \"num_questions\": len(sub_df),\n",
    "        \"accuracy\": acc_sub\n",
    "    })\n",
    "\n",
    "# Crea un DataFrame\n",
    "subject_df = pd.DataFrame(subject_stats).sort_values(by=\"accuracy\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Mostra\n",
    "print(\"\\nüìä Accuracy per subject:\")\n",
    "subject_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cd1d5e-1b82-44cb-b48f-a49ffcd661ca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c730fe0-bbcf-4816-9184-6987625439e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  BASE: 0.7716 | EA: 0.7702 | Œî (EA-BASE): -0.0014\n",
      "Contingency: [[a=10421, b=414], [c=394, d=2813]]\n",
      "McNemar p-value: 0.503867  (chi2+cc)\n"
     ]
    }
   ],
   "source": [
    "# Statistic tests\n",
    "\n",
    "import pandas as pd\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "# 1) Carica le due run (gi√† fatto)\n",
    "df_run_base = pd.read_csv(\"../data/processed/rag/base_mmlu_rag_output_cut_5.tsv\", sep=\"\\t\")\n",
    "df_run_ea   = pd.read_csv(\"../data/processed/rag/ea_mmlu_rag_output_cut_5.tsv\",   sep=\"\\t\")\n",
    "\n",
    "# 2) Normalizza colonne e chiave d‚Äôallineamento\n",
    "def norm_ans(x):\n",
    "    m = {0:\"A\",1:\"B\",2:\"C\",3:\"D\"}\n",
    "    s = str(x).strip().upper()\n",
    "    return m.get(int(s), s) if s.isdigit() and int(s) in m else s\n",
    "\n",
    "for df in (df_run_base, df_run_ea):\n",
    "    df[\"qanswer\"] = df[\"qanswer\"].map(norm_ans)\n",
    "    df[\"gold_answer\"] = df[\"gold_answer\"].map(norm_ans)\n",
    "\n",
    "key = \"question_id\" if \"question_id\" in df_run_base.columns and \"question_id\" in df_run_ea.columns else \\\n",
    "      \"question\"     if \"question\"     in df_run_base.columns and \"question\"     in df_run_ea.columns else None\n",
    "if key is None:\n",
    "    df_run_base = df_run_base.reset_index().rename(columns={\"index\":\"row_id\"})\n",
    "    df_run_ea   = df_run_ea.reset_index().rename(columns={\"index\":\"row_id\"})\n",
    "    key = \"row_id\"\n",
    "\n",
    "df = df_run_base[[key,\"qanswer\",\"gold_answer\"]].merge(\n",
    "        df_run_ea[[key,\"qanswer\"]], on=key, suffixes=(\"_base\",\"_ea\"), how=\"inner\"\n",
    "     )\n",
    "\n",
    "# 3) Accuracy per modello\n",
    "acc_base = (df[\"qanswer_base\"] == df[\"gold_answer\"]).mean()\n",
    "acc_ea   = (df[\"qanswer_ea\"]   == df[\"gold_answer\"]).mean()\n",
    "delta    = acc_ea - acc_base\n",
    "print(f\"Accuracy  BASE: {acc_base:.4f} | EA: {acc_ea:.4f} | Œî (EA-BASE): {delta:.4f}\")\n",
    "\n",
    "# 4) Tabella 2x2 per McNemar\n",
    "base_ok = df[\"qanswer_base\"] == df[\"gold_answer\"]\n",
    "ea_ok   = df[\"qanswer_ea\"]   == df[\"gold_answer\"]\n",
    "\n",
    "a = int(( base_ok &  ea_ok).sum())  # entrambi corretti\n",
    "b = int(( base_ok & ~ea_ok).sum())  # BASE solo corretto\n",
    "c = int((~base_ok &  ea_ok).sum())  # EA   solo corretto\n",
    "d = int((~base_ok & ~ea_ok).sum())  # entrambi sbagliati\n",
    "table = [[a, b],[c, d]]\n",
    "print(f\"Contingency: [[a={a}, b={b}], [c={c}, d={d}]]\")\n",
    "\n",
    "# 5) McNemar (esatto se b+c piccolo, altrimenti chi-quadro con correzione di continuit√†)\n",
    "use_exact = (b + c) <= 25\n",
    "res = mcnemar(table, exact=use_exact, correction=not use_exact)\n",
    "print(f\"McNemar p-value: {res.pvalue:.6g}  ({'exact' if use_exact else 'chi2+cc'})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8437d-1a3a-442e-86bc-dd1b14806c21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ptrag]",
   "language": "python",
   "name": "conda-env-ptrag-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
