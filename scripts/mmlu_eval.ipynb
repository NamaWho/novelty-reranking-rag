{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cefb03b-ca0a-485c-a93e-ad12bf4a1387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyterrier as pt\n",
    "import pyterrier_rag as ptr\n",
    "from datasets import load_dataset\n",
    "import ir_datasets\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b9b8566-7a7a-42ee-b1fb-1029d3f53d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Carico MMLU da Hugging Face...\n",
      "üîç Carico ranking esistente...\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 2. Caricamento dataset MMLU\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"üîç Carico MMLU da Hugging Face...\")\n",
    "mmlu = load_dataset(\"cais/mmlu\", \"all\", split=\"test\")  # HF loader :contentReference[oaicite:7]{index=7}\n",
    "# Rinomina e prepara DataFrame\n",
    "#df_mmlu = pd.DataFrame(mmlu) \\\n",
    "#    .rename(columns={\"question\": \"query\", \"answer\": \"gold_answer\"}) \\\n",
    "#    .assign(qid=lambda df: df.index.astype(str))\n",
    "\n",
    "\n",
    "# Costruisci il DataFrame e imposta il nuovo qid = subject + \"_\" + index\n",
    "df_mmlu = (\n",
    "    pd.DataFrame(mmlu)\n",
    "      # crea qid unendo subject e index\n",
    "      .assign(qid=lambda df: df[\"subject\"] + \"_\" + df.index.astype(str))\n",
    "      # rinomina le colonne per lo script RAG\n",
    "      .rename(columns={\n",
    "          \"question\": \"query\",\n",
    "          \"answer\": \"gold_answer\"\n",
    "      })\n",
    ")\n",
    "\n",
    "print(\"üîç Carico ranking esistente...\")\n",
    "df_run_base = pd.read_csv(\n",
    "    \"../data/processed/rag/__setencoder-novelty-base__msmarco-segment-mmlu.tsv\", sep=\"\\t\",\n",
    "    names=[\"qid\", \"Q0\", \"doc_id\", \"rank\", \"score\", \"run_name\", \"text\"]\n",
    ")\n",
    "df_run_ea = pd.read_csv(\n",
    "    \"../data/processed/rag/__setencoder-novelty-ea__msmarco-segment-mmlu.tsv\", sep=\"\\t\",\n",
    "    names=[\"qid\", \"Q0\", \"doc_id\", \"rank\", \"score\", \"run_name\", \"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9f097ec-5fab-4db1-aa00-c3b8b803adaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 113520750/113520750 [43:27<00:00, 43539.88it/s] \n"
     ]
    }
   ],
   "source": [
    "dataset = ir_datasets.load('msmarco-segment-v2.1')\n",
    "pt_dataset = pt.get_dataset(\"irds:msmarco-segment-v2.1\")\n",
    "total_docs = dataset.docs_count() \n",
    "all_docs = list(tqdm(dataset.docs_iter(), total=total_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5539f99c-d015-4e93-ba48-e45392827e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict = {doc.doc_id: doc for doc in all_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad24324e-d41a-4dd5-8330-76ded4e6d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier_rag.backend import OpenAIBackend\n",
    "from pyterrier_rag.prompt import Concatenator\n",
    "from pyterrier_rag.readers import Reader\n",
    "from pyterrier_rag.prompt import PromptTransformer, prompt\n",
    "from fastchat.model import get_conversation_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1857ec0-22cb-4085-9f6e-eb74c21ec5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"llama-3-8b-instruct\"\n",
    "model_name = \"llama-3.3-70b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae4aad4c-346c-40af-a0c3-4a4a0b076073",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = r\"\"\"You are an expert Q&A system that is trusted around the world. \n",
    "        Always answer the query using the provided context information,\n",
    "        and not prior knowledge.\n",
    "\n",
    "        Some rules to follow:\n",
    "        1. Never directly reference the given context in your answer\n",
    "        2. Avoid statements like 'Based on the context, ...' or \n",
    "        'The context information ...' or anything along those lines.\n",
    "        3. Output must be a single uppercase letter: A, B, C, or D ‚Äî nothing else.\"\"\"\n",
    "prompt_text = \"\"\"Context information is below.\n",
    "            ---------------------\n",
    "            {{ qcontext }}\n",
    "            ---------------------\n",
    "            Given the context information and a multiple-choice question, choose the correct answer.\n",
    "            \n",
    "            Query: {{ query }}\n",
    "\n",
    "            Choices: {{ choices }}\n",
    "\n",
    "            Answer with only the letter [\"A\", \"B\", \"C\", \"D\"] corresponding to the correct choice, with no mention of \"\". Do not include any explanation or additional text.\n",
    "\n",
    "            Answer: \"\"\"\n",
    "\n",
    "template = get_conversation_template(\"meta-llama-3.1-sp\")\n",
    "prompt = PromptTransformer(\n",
    "    conversation_template=template,\n",
    "    system_message=system_message,\n",
    "    instruction=prompt_text,\n",
    "    input_fields=[\"query\", \"qcontext\", 'choices'],\n",
    "    api_type=\"openai\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c8cfc4c-c402-4586-949f-7e3d56e5f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank(df_queries):\n",
    "    run = df_queries.merge(df_run_base, on=\"qid\", how=\"left\")\n",
    "    return run\n",
    "get_rank_pipe = pt.apply.generic(get_rank)\n",
    "\n",
    "def get_rank_common_docs(df_queries, k=5):\n",
    "    rows = []\n",
    "\n",
    "    for _, row in df_queries.iterrows():\n",
    "        qid = row[\"qid\"]\n",
    "        docs_base = set(df_run_base[df_run_base[\"qid\"] == qid].sort_values(\"rank\").head(k)[\"doc_id\"])\n",
    "        docs_ea = set(df_run_ea[df_run_ea[\"qid\"] == qid].sort_values(\"rank\").head(k)[\"doc_id\"])\n",
    "\n",
    "        common_docs = docs_base & docs_ea\n",
    "\n",
    "        if len(common_docs) == 0:\n",
    "            final_docs = list(docs_ea)\n",
    "        else:\n",
    "            n_missing = k - len(common_docs)\n",
    "    \n",
    "            random_docs = []\n",
    "            while len(random_docs) < n_missing:\n",
    "                random_doc = all_docs[np.random.randint(0, len(all_docs))]\n",
    "                if random_doc.doc_id not in docs_base and random_doc.doc_id not in docs_ea:\n",
    "                    print(random_doc.doc_id)\n",
    "                    random_docs.append(random_doc.doc_id)\n",
    "    \n",
    "            final_docs = list(common_docs) + random_docs\n",
    "        \n",
    "        for rank, doc_id in enumerate(final_docs):\n",
    "            rows.append({\n",
    "                \"query\": row['query'],\n",
    "                \"qid\": qid,\n",
    "                \"doc_id\": doc_id,\n",
    "                \"rank\": rank,\n",
    "                \"score\": 1.0 - 0.01 * rank,  # dummy score\n",
    "                \"text\": doc_dict.get(doc_id).segment, \n",
    "                \"choices\": row['choices']\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "get_rank_commondocs_pipe = pt.apply.generic(get_rank_common_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78513c1f-8861-459e-bb9c-5c96f1fb78fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"casperhansen/llama-3-8b-instruct-awq\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"casperhansen/llama-3.3-70b-instruct-awq\")\n",
    "\n",
    "generation_args={\n",
    "    \"temperature\": 0.01,\n",
    "    \"max_tokens\": 1,\n",
    "}\n",
    "\n",
    "# this could equally be a real OpenAI models\n",
    "llama = OpenAIBackend(model_name, \n",
    "                      api_key=os.environ['IDA_LLM_API_KEY'],\n",
    "                      generation_args=generation_args,\n",
    "                      base_url=\"http://api.llm.apps.os.dcs.gla.ac.uk/v1\", \n",
    "                      verbose=True, \n",
    "                      parallel=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6058c64-f79a-46f5-8f12-08dcec909ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_reader = Reader(llama, prompt=prompt)\n",
    "#set_encoder_llama = get_rank_pipe % 3 >> Concatenator(tokenizer=tokenizer, max_length=8191,max_per_context=819,additional_fields=[\"choices\"]) >> llama_reader\n",
    "set_encoder_llama = get_rank_commondocs_pipe % 5 >> Concatenator(tokenizer=tokenizer, max_length=8191,max_per_context=2730,additional_fields=[\"choices\"]) >> llama_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27ba5dd0-dec5-4844-b2a7-428f2f4489f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® Nessun salvataggio trovato, si parte da zero.\n",
      "üß† Da processare: 14042 esempi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÅ RAG on MMLU:   0%|          | 1/14042 [00:51<201:08:34, 51.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 503 - {'error': 'llama-3.3-70b-instruct is starting up. Please wait a minute and try again.'}\n",
      "msmarco_v2.1_doc_23_784101806#11_1731332762\n",
      "msmarco_v2.1_doc_50_2286475756#1_3087351484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÅ RAG on MMLU:   0%|          | 2/14042 [01:42<198:45:31, 50.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 503 - {'error': 'llama-3.3-70b-instruct is starting up. Please wait a minute and try again.'}\n",
      "msmarco_v2.1_doc_17_851997906#11_937656270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÅ RAG on MMLU:   0%|          | 3/14042 [02:29<191:32:43, 49.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 503 - {'error': 'llama-3.3-70b-instruct is starting up. Please wait a minute and try again.'}\n",
      "msmarco_v2.1_doc_14_172303228#14_393715007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÅ RAG on MMLU:   0%|          | 4/14042 [03:19<192:52:14, 49.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 503 - {'error': 'llama-3.3-70b-instruct is starting up. Please wait a minute and try again.'}\n",
      "msmarco_v2.1_doc_31_1421966689#8_2886213402\n",
      "msmarco_v2.1_doc_34_893571898#3_1837576901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÅ RAG on MMLU:   0%|          | 5/14042 [04:08<192:32:59, 49.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 503 - {'error': 'llama-3.3-70b-instruct is starting up. Please wait a minute and try again.'}\n",
      "msmarco_v2.1_doc_57_250153360#10_542239871\n",
      "msmarco_v2.1_doc_21_1495833963#8_3276351120\n",
      "msmarco_v2.1_doc_13_1085699448#2_2409530484\n",
      "msmarco_v2.1_doc_32_547801233#3_1043329466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÅ RAG on MMLU:   0%|          | 6/14042 [04:54<188:17:05, 48.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 503 - {'error': 'llama-3.3-70b-instruct is starting up. Please wait a minute and try again.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÅ RAG on MMLU:   0%|          | 7/14042 [05:41<187:01:14, 47.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 503 - {'error': 'llama-3.3-70b-instruct is starting up. Please wait a minute and try again.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÅ RAG on MMLU:   0%|          | 8/14042 [06:31<188:43:10, 48.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 503 - {'error': 'llama-3.3-70b-instruct is starting up. Please wait a minute and try again.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÅ RAG on MMLU:   0%|          | 9/14042 [07:18<187:46:21, 48.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 503 - {'error': 'llama-3.3-70b-instruct is starting up. Please wait a minute and try again.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÅ RAG on MMLU:   0%|          | 10/14042 [07:56<175:11:55, 44.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msmarco_v2.1_doc_11_1296760589#2_2705118181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÅ RAG on MMLU:   0%|          | 11/14042 [07:58<123:50:26, 31.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msmarco_v2.1_doc_05_1445117423#9_2773625560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÅ RAG on MMLU:   0%|          | 11/14042 [07:58<169:38:32, 43.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 8192 tokens. However, you requested 8385 tokens (8384 in the messages, 1 in the completion). Please reduce the length of the messages or completion. None\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'num_responses'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/primary/projects/pyterrier_rag/pyterrier_rag/backend/_openai.py:136\u001b[0m, in \u001b[0;36mOpenAIBackend._call_chat_completion\u001b[0;34m(self, messages, max_new_tokens, return_logprobs, num_responses)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     completions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ptrag/lib/python3.10/site-packages/openai/_utils/_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ptrag/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:925\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    924\u001b[0m validate_response_format(response_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ptrag/lib/python3.10/site-packages/openai/_base_client.py:1249\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1246\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1247\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1248\u001b[0m )\n\u001b[0;32m-> 1249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ptrag/lib/python3.10/site-packages/openai/_base_client.py:1037\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1036\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1037\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 8192 tokens. However, you requested 8385 tokens (8384 in the messages, 1 in the completion). Please reduce the length of the messages or completion. None\", 'type': 'BadRequestError', 'param': None, 'code': 400}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tqdm(remaining\u001b[38;5;241m.\u001b[39miterrows(), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(remaining), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîÅ RAG on MMLU\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     23\u001b[0m     idx, data \u001b[38;5;241m=\u001b[39m row\n\u001b[0;32m---> 24\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mset_encoder_llama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     result_merged \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mmerge(df_mmlu[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgold_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m]], on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result_merged)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ptrag/lib/python3.10/site-packages/pyterrier/_ops.py:372\u001b[0m, in \u001b[0;36mCompose.transform\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    370\u001b[0m out \u001b[38;5;241m=\u001b[39m inp\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformers:\n\u001b[0;32m--> 372\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/mnt/primary/projects/pyterrier_rag/pyterrier_rag/readers/_base.py:57\u001b[0m, in \u001b[0;36mReader.transform\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp: pd\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m     56\u001b[0m     prompts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt(inp)\n\u001b[0;32m---> 57\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     answers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39manswer_extraction(outputs)\n\u001b[1;32m     60\u001b[0m     prompts[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_field] \u001b[38;5;241m=\u001b[39m answers[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_field]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ptrag/lib/python3.10/site-packages/pyterrier/transformer.py:142\u001b[0m, in \u001b[0;36mTransformer.__call__\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Runs the transformer for the given input and returns its output as the same type as the input.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m    :rtype: ``pd.DataFrame``, ``Iterable[Dict]``, ``List[Dict]``\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inp, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_iter(inp)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inp, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ptrag/lib/python3.10/site-packages/pyterrier/transformer.py:101\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    Abstract method that runs the transformer over Pandas ``DataFrame`` objects. This or :meth:`transform_iter`\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    must be implemented by all Transformer objects.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    :rtype: ``pd.DataFrame``\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# We should have no recursive transform <-> transform_iter problem, due to the __new__ check.\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/mnt/primary/projects/pyterrier_rag/pyterrier_rag/backend/_base.py:212\u001b[0m, in \u001b[0;36mTextGenerator.transform_iter\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    210\u001b[0m chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunk)\n\u001b[1;32m    211\u001b[0m prompts \u001b[38;5;241m=\u001b[39m [i[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_field] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m chunk]\n\u001b[0;32m--> 212\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_logprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogprobs_field\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_responses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_responses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, rec \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunk):\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_responses):\n",
      "File \u001b[0;32m/mnt/primary/projects/pyterrier_rag/pyterrier_rag/backend/_openai.py:188\u001b[0m, in \u001b[0;36mOpenAIBackend.generate\u001b[0;34m(self, inps, return_logprobs, max_new_tokens, num_responses)\u001b[0m\n\u001b[1;32m    186\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[0;32m--> 188\u001b[0m     results\u001b[38;5;241m.\u001b[39mextend(\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ptrag/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ptrag/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ptrag/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/mnt/primary/projects/pyterrier_rag/pyterrier_rag/backend/_openai.py:140\u001b[0m, in \u001b[0;36mOpenAIBackend._call_chat_completion\u001b[0;34m(self, messages, max_new_tokens, return_logprobs, num_responses)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(e))\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms maximum context length is\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [BackendOutput(text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR::reduce_length\u001b[39m\u001b[38;5;124m\"\u001b[39m)] \u001b[38;5;241m*\u001b[39m \u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_responses\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe response was filtered\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [BackendOutput(text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR::response_filtered\u001b[39m\u001b[38;5;124m\"\u001b[39m)] \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mnum_responses\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'num_responses'"
     ]
    }
   ],
   "source": [
    "#print(\"‚öôÔ∏è Esecuzione pipeline RAG su MMLU‚Ä¶\")\n",
    "#results = set_encoder_llama.transform(df_mmlu.head(1000))\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "save_every=100\n",
    "partial_save_path=\"../data/processed/rag/commondocs_mmlu_rag_output_cut_5.tsv\"\n",
    "\n",
    "try:\n",
    "    df_partial = pd.read_csv(partial_save_path, sep=\"\\t\")\n",
    "    done_qids = set(df_partial[\"qid\"])\n",
    "    print(f\"‚úÖ Ripresi {len(done_qids)} risultati da salvataggio parziale.\")\n",
    "    results = [df_partial]\n",
    "except FileNotFoundError:\n",
    "    print(\"üö® Nessun salvataggio trovato, si parte da zero.\")\n",
    "    done_qids = set()\n",
    "    results = []\n",
    "\n",
    "remaining = df_mmlu[~df_mmlu[\"qid\"].isin(done_qids)].reset_index(drop=True)\n",
    "print(f\"üß† Da processare: {len(remaining)} esempi.\")\n",
    "    \n",
    "for row in tqdm(remaining.iterrows(), total=len(remaining), desc=\"üîÅ RAG on MMLU\"):\n",
    "    idx, data = row\n",
    "    result = set_encoder_llama.transform(pd.DataFrame([data]))\n",
    "\n",
    "    result_merged = result.merge(df_mmlu[[\"qid\", \"gold_answer\"]], on=\"qid\", how=\"left\")\n",
    "    results.append(result_merged)\n",
    "\n",
    "    # ‚úÖ Salvataggio intermedio\n",
    "    if (idx + 1) % save_every == 0 or (idx + 1) == len(remaining):\n",
    "        df_save = pd.concat(results, ignore_index=True)\n",
    "        df_save.to_csv(partial_save_path, sep=\"\\t\", index=False)\n",
    "        print(f\"üíæ Salvati {len(df_save)} risultati su {partial_save_path}\")\n",
    "        results = [df_save]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa370b94-c10b-4888-b953-26be27c8749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 6. Valutazione\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "if isinstance(results, list):\n",
    "    results = pd.concat(results, ignore_index=True)\n",
    "    \n",
    "# üîÅ Converte i valori numerici in lettere\n",
    "index_to_choice = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
    "results[\"gold_answer\"] = results[\"gold_answer\"].map(index_to_choice)\n",
    "\n",
    "def evaluate(preds, golds):\n",
    "    preds = [str(p).strip().lower() for p in preds]\n",
    "    golds = [str(g).strip().lower() for g in golds]\n",
    "    return accuracy_score(golds, preds)\n",
    "\n",
    "acc = evaluate(results[\"qanswer\"], results[\"gold_answer\"])\n",
    "print(f\"\\nüìä MMLU Accuracy (EM): {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b47a8d3-6d83-4b4b-9b51-2fa79cd4bdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrai il subject da qid\n",
    "results[\"subject\"] = results[\"qid\"].apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n",
    "\n",
    "# Calcola accuracy per subject e raccoglila in una lista di dizionari\n",
    "subject_stats = []\n",
    "\n",
    "for subject in sorted(results[\"subject\"].unique()):\n",
    "    sub_df = results[results[\"subject\"] == subject]\n",
    "    acc_sub = evaluate(sub_df[\"qanswer\"], sub_df[\"gold_answer\"])\n",
    "    subject_stats.append({\n",
    "        \"subject\": subject,\n",
    "        \"num_questions\": len(sub_df),\n",
    "        \"accuracy\": acc_sub\n",
    "    })\n",
    "\n",
    "# Crea un DataFrame\n",
    "subject_df = pd.DataFrame(subject_stats).sort_values(by=\"accuracy\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Mostra\n",
    "print(\"\\nüìä Accuracy per subject:\")\n",
    "subject_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f55baa-6a66-4113-9678-d53d664b8582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ptrag]",
   "language": "python",
   "name": "conda-env-ptrag-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
