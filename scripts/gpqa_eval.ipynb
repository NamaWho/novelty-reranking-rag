{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cefb03b-ca0a-485c-a93e-ad12bf4a1387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyterrier as pt\n",
    "import pyterrier_rag as ptr\n",
    "from datasets import load_dataset\n",
    "import ir_datasets\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9b8566-7a7a-42ee-b1fb-1029d3f53d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 2. Caricamento dataset GPQA\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"üîç Carico GPQA da Hugging Face...\")\n",
    "mmlu = load_dataset(\"Idavidrein/gpqa\", \"gpqa_diamond\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f097ec-5fab-4db1-aa00-c3b8b803adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ir_datasets.load('msmarco-segment-v2.1')\n",
    "pt_dataset = pt.get_dataset(\"irds:msmarco-segment-v2.1\")\n",
    "total_docs = dataset.docs_count() \n",
    "all_docs = list(tqdm(dataset.docs_iter(), total=total_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5539f99c-d015-4e93-ba48-e45392827e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict = {doc.doc_id: doc for doc in all_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad24324e-d41a-4dd5-8330-76ded4e6d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier_rag.backend import OpenAIBackend\n",
    "from pyterrier_rag.prompt import Concatenator\n",
    "from pyterrier_rag.readers import Reader\n",
    "from pyterrier_rag.prompt import PromptTransformer, prompt\n",
    "from fastchat.model import get_conversation_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1857ec0-22cb-4085-9f6e-eb74c21ec5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"llama-3-8b-instruct\"\n",
    "model_name = \"llama-3.3-70b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4aad4c-346c-40af-a0c3-4a4a0b076073",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = r\"\"\"You are an expert Q&A system that is trusted around the world. \n",
    "        Always answer the query using the provided context information,\n",
    "        and not prior knowledge.\n",
    "\n",
    "        Some rules to follow:\n",
    "        1. Never directly reference the given context in your answer\n",
    "        2. Avoid statements like 'Based on the context, ...' or \n",
    "        'The context information ...' or anything along those lines.\n",
    "        3. Output must be a single uppercase letter: A, B, C, or D ‚Äî nothing else.\"\"\"\n",
    "prompt_text = \"\"\"Context information is below.\n",
    "            ---------------------\n",
    "            {{ qcontext }}\n",
    "            ---------------------\n",
    "            Given the context information and a multiple-choice question, choose the correct answer.\n",
    "            \n",
    "            Query: {{ query }}\n",
    "\n",
    "            Choices: {{ choices }}\n",
    "\n",
    "            Answer with only the letter [\"A\", \"B\", \"C\", \"D\"] corresponding to the correct choice, with no mention of \"\". Do not include any explanation or additional text.\n",
    "\n",
    "            Answer: \"\"\"\n",
    "\n",
    "template = get_conversation_template(\"meta-llama-3.1-sp\")\n",
    "prompt = PromptTransformer(\n",
    "    conversation_template=template,\n",
    "    system_message=system_message,\n",
    "    instruction=prompt_text,\n",
    "    input_fields=[\"query\", \"qcontext\", 'choices'],\n",
    "    api_type=\"openai\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8cfc4c-c402-4586-949f-7e3d56e5f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank(df_queries):\n",
    "    run = df_queries.merge(df_run_base, on=\"qid\", how=\"left\")\n",
    "    return run\n",
    "get_rank_pipe = pt.apply.generic(get_rank)\n",
    "\n",
    "def get_rank_common_docs(df_queries, k=5):\n",
    "    rows = []\n",
    "\n",
    "    for _, row in df_queries.iterrows():\n",
    "        qid = row[\"qid\"]\n",
    "        docs_base = set(df_run_base[df_run_base[\"qid\"] == qid].sort_values(\"rank\").head(k)[\"doc_id\"])\n",
    "        docs_ea = set(df_run_ea[df_run_ea[\"qid\"] == qid].sort_values(\"rank\").head(k)[\"doc_id\"])\n",
    "\n",
    "        common_docs = docs_base & docs_ea\n",
    "\n",
    "        if len(common_docs) == 0:\n",
    "            final_docs = list(docs_ea)\n",
    "        else:\n",
    "            n_missing = k - len(common_docs)\n",
    "    \n",
    "            random_docs = []\n",
    "            while len(random_docs) < n_missing:\n",
    "                random_doc = all_docs[np.random.randint(0, len(all_docs))]\n",
    "                if random_doc.doc_id not in docs_base and random_doc.doc_id not in docs_ea:\n",
    "                    print(random_doc.doc_id)\n",
    "                    random_docs.append(random_doc.doc_id)\n",
    "    \n",
    "            final_docs = list(common_docs) + random_docs\n",
    "        \n",
    "        for rank, doc_id in enumerate(final_docs):\n",
    "            rows.append({\n",
    "                \"query\": row['query'],\n",
    "                \"qid\": qid,\n",
    "                \"doc_id\": doc_id,\n",
    "                \"rank\": rank,\n",
    "                \"score\": 1.0 - 0.01 * rank,  # dummy score\n",
    "                \"text\": doc_dict.get(doc_id).segment, \n",
    "                \"choices\": row['choices']\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "get_rank_commondocs_pipe = pt.apply.generic(get_rank_common_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78513c1f-8861-459e-bb9c-5c96f1fb78fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"casperhansen/llama-3-8b-instruct-awq\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"casperhansen/llama-3.3-70b-instruct-awq\")\n",
    "\n",
    "generation_args={\n",
    "    \"temperature\": 0.01,\n",
    "    \"max_tokens\": 1,\n",
    "}\n",
    "\n",
    "# this could equally be a real OpenAI models\n",
    "llama = OpenAIBackend(model_name, \n",
    "                      api_key=os.environ['IDA_LLM_API_KEY'],\n",
    "                      generation_args=generation_args,\n",
    "                      base_url=\"http://api.llm.apps.os.dcs.gla.ac.uk/v1\", \n",
    "                      verbose=True, \n",
    "                      parallel=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6058c64-f79a-46f5-8f12-08dcec909ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_reader = Reader(llama, prompt=prompt)\n",
    "#set_encoder_llama = get_rank_pipe % 3 >> Concatenator(tokenizer=tokenizer, max_length=8191,max_per_context=819,additional_fields=[\"choices\"]) >> llama_reader\n",
    "set_encoder_llama = get_rank_commondocs_pipe % 5 >> Concatenator(tokenizer=tokenizer, max_length=8191,max_per_context=2730,additional_fields=[\"choices\"]) >> llama_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba5dd0-dec5-4844-b2a7-428f2f4489f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"‚öôÔ∏è Esecuzione pipeline RAG su MMLU‚Ä¶\")\n",
    "#results = set_encoder_llama.transform(df_mmlu.head(1000))\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "save_every=100\n",
    "partial_save_path=\"../data/processed/rag/commondocs_mmlu_rag_output_cut_5.tsv\"\n",
    "\n",
    "try:\n",
    "    df_partial = pd.read_csv(partial_save_path, sep=\"\\t\")\n",
    "    done_qids = set(df_partial[\"qid\"])\n",
    "    print(f\"‚úÖ Ripresi {len(done_qids)} risultati da salvataggio parziale.\")\n",
    "    results = [df_partial]\n",
    "except FileNotFoundError:\n",
    "    print(\"üö® Nessun salvataggio trovato, si parte da zero.\")\n",
    "    done_qids = set()\n",
    "    results = []\n",
    "\n",
    "remaining = df_mmlu[~df_mmlu[\"qid\"].isin(done_qids)].reset_index(drop=True)\n",
    "print(f\"üß† Da processare: {len(remaining)} esempi.\")\n",
    "    \n",
    "for row in tqdm(remaining.iterrows(), total=len(remaining), desc=\"üîÅ RAG on MMLU\"):\n",
    "    idx, data = row\n",
    "    result = set_encoder_llama.transform(pd.DataFrame([data]))\n",
    "\n",
    "    result_merged = result.merge(df_mmlu[[\"qid\", \"gold_answer\"]], on=\"qid\", how=\"left\")\n",
    "    results.append(result_merged)\n",
    "\n",
    "    # ‚úÖ Salvataggio intermedio\n",
    "    if (idx + 1) % save_every == 0 or (idx + 1) == len(remaining):\n",
    "        df_save = pd.concat(results, ignore_index=True)\n",
    "        df_save.to_csv(partial_save_path, sep=\"\\t\", index=False)\n",
    "        print(f\"üíæ Salvati {len(df_save)} risultati su {partial_save_path}\")\n",
    "        results = [df_save]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa370b94-c10b-4888-b953-26be27c8749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 6. Valutazione\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "if isinstance(results, list):\n",
    "    results = pd.concat(results, ignore_index=True)\n",
    "    \n",
    "# üîÅ Converte i valori numerici in lettere\n",
    "index_to_choice = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
    "results[\"gold_answer\"] = results[\"gold_answer\"].map(index_to_choice)\n",
    "\n",
    "def evaluate(preds, golds):\n",
    "    preds = [str(p).strip().lower() for p in preds]\n",
    "    golds = [str(g).strip().lower() for g in golds]\n",
    "    return accuracy_score(golds, preds)\n",
    "\n",
    "acc = evaluate(results[\"qanswer\"], results[\"gold_answer\"])\n",
    "print(f\"\\nüìä MMLU Accuracy (EM): {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b47a8d3-6d83-4b4b-9b51-2fa79cd4bdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrai il subject da qid\n",
    "results[\"subject\"] = results[\"qid\"].apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n",
    "\n",
    "# Calcola accuracy per subject e raccoglila in una lista di dizionari\n",
    "subject_stats = []\n",
    "\n",
    "for subject in sorted(results[\"subject\"].unique()):\n",
    "    sub_df = results[results[\"subject\"] == subject]\n",
    "    acc_sub = evaluate(sub_df[\"qanswer\"], sub_df[\"gold_answer\"])\n",
    "    subject_stats.append({\n",
    "        \"subject\": subject,\n",
    "        \"num_questions\": len(sub_df),\n",
    "        \"accuracy\": acc_sub\n",
    "    })\n",
    "\n",
    "# Crea un DataFrame\n",
    "subject_df = pd.DataFrame(subject_stats).sort_values(by=\"accuracy\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Mostra\n",
    "print(\"\\nüìä Accuracy per subject:\")\n",
    "subject_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f55baa-6a66-4113-9678-d53d664b8582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
