{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cefb03b-ca0a-485c-a93e-ad12bf4a1387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/emalir-dr-rag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyterrier as pt\n",
    "import pyterrier_rag as ptr\n",
    "from datasets import load_dataset\n",
    "import ir_datasets\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pyterrier_alpha as pta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b9b8566-7a7a-42ee-b1fb-1029d3f53d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Carico GPQA da Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since Idavidrein/gpqa couldn't be found on the Hugging Face Hub\n",
      "WARNING:datasets.load:Using the latest cached version of the dataset since Idavidrein/gpqa couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'gpqa_diamond' at /root/.cache/huggingface/datasets/Idavidrein___gpqa/gpqa_diamond/0.0.0/90b8e5be2b1d3d2dbfe016cdab47981150600c4a (last modified on Sat Jul 26 13:40:15 2025).\n",
      "WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'gpqa_diamond' at /root/.cache/huggingface/datasets/Idavidrein___gpqa/gpqa_diamond/0.0.0/90b8e5be2b1d3d2dbfe016cdab47981150600c4a (last modified on Sat Jul 26 13:40:15 2025).\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 2. Caricamento dataset GPQA\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"🔍 Carico GPQA da Hugging Face...\")\n",
    "mmlu = load_dataset(\"Idavidrein/gpqa\", \"gpqa_diamond\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c46c13f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a DataFrame from the dataset with 'query' and 'gold_answer' columns\n",
    "df = pd.DataFrame(mmlu)\n",
    "df = df.rename(columns={\"Record ID\": \"qid\",\"Question\": \"query\", \"Correct Answer\": \"gold_answer\"})\n",
    "df = df[[\"qid\", \"query\", \"gold_answer\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b42c29-ddc4-4045-8007-4b2477bc186e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9380cc26-fab2-4345-a142-cef6132ca01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.rename(columns={\"qid\": \"qid\", \"query\": \"text\"})\n",
    "data = data[['qid', 'text']]\n",
    "data[\"text\"] = data[\"text\"].str.replace('\\n', '\\\\n')\n",
    "data.to_csv(\"../data/raw/rag/gpqa-queries.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82d6cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ir_datasets.load('msmarco-segment-v2.1')\n",
    "pt_dataset = pt.get_dataset(\"irds:msmarco-segment-v2.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b80435f6-7e85-4517-9f96-8694c46b5f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_segment(run):\n",
    "    run = run.rename(columns={\"segment\": \"text\"})\n",
    "    return run\n",
    "rename_pipe = pt.apply.generic(rename_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "859aa54b-1291-4326-b11f-d57d1d99cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pta.Artifact.from_hf('namawho/msmarco-segment-v2.1.pisa')\n",
    "bm25_ret = index.bm25(verbose=True) % 200 >> pt.text.get_text(pt_dataset, \"segment\") >> rename_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3abf3013-0033-457f-82d5-39d90ab63e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PISA bm25: 100%|██████████| 198/198 [07:17<00:00,  2.21s/query]\n"
     ]
    }
   ],
   "source": [
    "retrieved = bm25_ret.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f26b994-0674-48d0-891d-62cf9230606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved[\"Q0\"] = \"Q0\"\n",
    "retrieved[\"tag\"] = \"bm25\"\n",
    "retrieved = retrieved[[\"qid\",\"Q0\",\"docno\",\"rank\",\"score\",\"tag\"]]\n",
    "\n",
    "# scrivi su file\n",
    "retrieved.to_csv(\"../data/raw/rag/__bm25__msmarco-segment-gpqa.run\", sep=\" \", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b124e450-aabd-4592-881b-95d6617c75f0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eab8258f-3073-4ee8-a8c6-fc3507f8d3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Carico ranking esistente...\n"
     ]
    }
   ],
   "source": [
    "print(\"🔍 Carico ranking esistente...\")\n",
    "df_run_base = pd.read_csv(\n",
    "    \"../data/processed/rag/__setencoder-novelty-base__msmarco-segment-gpqa.tsv\", sep=\"\\t\",\n",
    "    names=[\"qid\", \"Q0\", \"docno\", \"rank\", \"score\", \"run_name\", \"text\"]\n",
    ")\n",
    "df_run_ea = pd.read_csv(\n",
    "    \"../data/processed/rag/__setencoder-novelty-ea__msmarco-segment-gpqa.tsv\", sep=\"\\t\",\n",
    "    names=[\"qid\", \"Q0\", \"docno\", \"rank\", \"score\", \"run_name\", \"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f097ec-5fab-4db1-aa00-c3b8b803adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ir_datasets.load('msmarco-segment-v2.1')\n",
    "pt_dataset = pt.get_dataset(\"irds:msmarco-segment-v2.1\")\n",
    "#total_docs = dataset.docs_count() \n",
    "#all_docs = list(tqdm(dataset.docs_iter(), total=total_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0ad54ec-68e7-4852-a06a-61dc3581b8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier_alpha as pta\n",
    "index = pta.Artifact.from_hf('namawho/msmarco-segment-v2.1.pisa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5539f99c-d015-4e93-ba48-e45392827e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_dict = {doc.doc_id: doc for doc in all_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad24324e-d41a-4dd5-8330-76ded4e6d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier_rag.backend import OpenAIBackend\n",
    "from pyterrier_rag.prompt import Concatenator\n",
    "from pyterrier_rag.readers import Reader\n",
    "from pyterrier_rag.prompt import PromptTransformer, prompt\n",
    "from fastchat.model import get_conversation_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1857ec0-22cb-4085-9f6e-eb74c21ec5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"llama-3-8b-instruct\"\n",
    "model_name = \"llama-3.3-70b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae4aad4c-346c-40af-a0c3-4a4a0b076073",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = r\"\"\"You are an expert Q&A system that is trusted around the world. \n",
    "        Always answer the query using the provided context information, and not prior knowledge.\n",
    "\n",
    "        Some rules to follow:\n",
    "        1. Never directly reference the given context in your answer\n",
    "        2. Avoid statements like 'Based on the context, ...' or \n",
    "        'The context information ...' or anything along those lines.\"\"\"\n",
    "prompt_text = \"\"\"Context information is below.\n",
    "            ---------------------\n",
    "            {{ qcontext }}\n",
    "            ---------------------\n",
    "            Given the context information, answer to the given query.\n",
    "            \n",
    "            Query: {{ query }}\n",
    "\n",
    "            Do not include any explanation or additional text.\n",
    "\n",
    "            Answer: \"\"\"\n",
    "\n",
    "template = get_conversation_template(\"meta-llama-3.1-sp\")\n",
    "prompt = PromptTransformer(\n",
    "    conversation_template=template,\n",
    "    system_message=system_message,\n",
    "    instruction=prompt_text,\n",
    "    input_fields=[\"query\", \"qcontext\"],\n",
    "    api_type=\"openai\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c8cfc4c-c402-4586-949f-7e3d56e5f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank(df_queries):\n",
    "    run = df_queries.merge(df_run_ea, on=\"qid\", how=\"left\")\n",
    "    return run\n",
    "get_rank_pipe = pt.apply.generic(get_rank)\n",
    "\n",
    "def rename_segment(run):\n",
    "    run = run.rename(columns={\"segment\": \"text\"})\n",
    "    return run\n",
    "rename_pipe = pt.apply.generic(rename_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78513c1f-8861-459e-bb9c-5c96f1fb78fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"casperhansen/llama-3-8b-instruct-awq\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"casperhansen/llama-3.3-70b-instruct-awq\")\n",
    "\n",
    "generation_args={\n",
    "    \"temperature\": 0.6,\n",
    "    \"max_tokens\": 140,\n",
    "}\n",
    "\n",
    "# this could equally be a real OpenAI models\n",
    "llama = OpenAIBackend(model_name, \n",
    "                      api_key=os.environ['IDA_LLM_API_KEY'],\n",
    "                      generation_args=generation_args,\n",
    "                      base_url=\"http://api.llm.apps.os.dcs.gla.ac.uk/v1\", \n",
    "                      verbose=True, \n",
    "                      parallel=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6058c64-f79a-46f5-8f12-08dcec909ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_reader = Reader(llama, prompt=prompt)\n",
    "set_encoder_llama = get_rank_pipe % 5 >>  pt.text.get_text(pt_dataset, \"segment\") >> rename_pipe >> Concatenator(tokenizer=tokenizer, max_length=8191,max_per_context=1638) >> llama_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35b0fd32-e25e-47bf-b66e-2b8634cbf0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/emalir-dr-rag/lib/python3.10/site-packages/pyterrier/transformer.py:101: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  return pd.DataFrame(list(self.transform_iter(inp.to_dict(orient='records'))))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>qid</th>\n",
       "      <th>query_0</th>\n",
       "      <th>qanswer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are an exp...</td>\n",
       "      <td>rec06pnAkLOr2t2mp</td>\n",
       "      <td>Two quantum states with energies E1 and E2 hav...</td>\n",
       "      <td>ΔE = 6.58 * 10^-4 eV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt                qid  \\\n",
       "0  [{'role': 'system', 'content': 'You are an exp...  rec06pnAkLOr2t2mp   \n",
       "\n",
       "                                             query_0               qanswer  \n",
       "0  Two quantum states with energies E1 and E2 hav...  ΔE = 6.58 * 10^-4 eV  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_encoder_llama.transform(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba5dd0-dec5-4844-b2a7-428f2f4489f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Nessun salvataggio trovato, si parte da zero.\n",
      "🧠 Da processare: 198 esempi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔁 RAG on MMLU:  51%|█████     | 100/198 [03:54<01:55,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Salvati 100 risultati su ../data/processed/rag/ea_gpqa_rag_output_cut_5.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔁 RAG on MMLU:  84%|████████▍ | 166/198 [07:10<02:14,  4.20s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "save_every=100\n",
    "partial_save_path=\"../data/processed/rag/ea_gpqa_rag_output_cut_5.tsv\"\n",
    "\n",
    "try:\n",
    "    df_partial = pd.read_csv(partial_save_path, sep=\"\\t\")\n",
    "    done_qids = set(df_partial[\"qid\"])\n",
    "    print(f\"✅ Ripresi {len(done_qids)} risultati da salvataggio parziale.\")\n",
    "    results = [df_partial]\n",
    "except FileNotFoundError:\n",
    "    print(\"🚨 Nessun salvataggio trovato, si parte da zero.\")\n",
    "    done_qids = set()\n",
    "    results = []\n",
    "\n",
    "remaining = df[~df[\"qid\"].isin(done_qids)].reset_index(drop=True)\n",
    "print(f\"🧠 Da processare: {len(remaining)} esempi.\")\n",
    "    \n",
    "for row in tqdm(remaining.iterrows(), total=len(remaining), desc=\"🔁 RAG on MMLU\"):\n",
    "    idx, data = row\n",
    "    result = set_encoder_llama.transform(pd.DataFrame([data]))\n",
    "\n",
    "    result_merged = result.merge(df[[\"qid\", \"gold_answer\"]], on=\"qid\", how=\"left\")\n",
    "    results.append(result_merged)\n",
    "\n",
    "    # ✅ Salvataggio intermedio\n",
    "    if (idx + 1) % save_every == 0 or (idx + 1) == len(remaining):\n",
    "        df_save = pd.concat(results, ignore_index=True)\n",
    "        df_save.to_csv(partial_save_path, sep=\"\\t\", index=False)\n",
    "        print(f\"💾 Salvati {len(df_save)} risultati su {partial_save_path}\")\n",
    "        results = [df_save]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95f55baa-6a66-4113-9678-d53d664b8582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt.Experiment: 100%|██████████| 8/8 [05:35<00:00, 41.99s/batches]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>F1</th>\n",
       "      <th>EM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>set encoder</td>\n",
       "      <td>0.127162</td>\n",
       "      <td>0.035354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name        F1        EM\n",
       "0  set encoder  0.127162  0.035354"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.Experiment(\n",
    "    [set_encoder_llama],\n",
    "    df[['qid', 'query']], # NB: remove .head() to run on all dev topics\n",
    "    df[['qid', 'gold_answer']],\n",
    "    [ptr.measures.F1, ptr.measures.EM],\n",
    "    batch_size=25,\n",
    "    verbose=True,\n",
    "    names=['set encoder'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0122d3-7711-4521-910e-ec41542c8cfb",
   "metadata": {},
   "source": [
    "# Score@3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d732b61-9df5-42ec-bfbc-a20f02d3d965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name\tF1\tEM\n",
    "# set encoder base\t0.134688\t0.040404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec8b58-3c3b-47ad-99a9-5ac934e924e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name\tF1\tEM\n",
    "# 0\tset encoder ea\t0.133429\t0.040404"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566e4471-bc6d-478a-8c30-a3770160d39a",
   "metadata": {},
   "source": [
    "# Score@5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c7f47-42da-4092-ae9d-eca702576402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \tname\tF1\tEM\n",
    "#   set encoder\tbase 0.128475\t0.035354\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aef7ea0-e43e-44ed-a7cb-dba5eaea7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name\tF1\tEM\n",
    "# 0\tset encoder ea\t0.127162\t0.035354"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dc9799-bbbb-49dc-aed3-7cf484ac5f24",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a21af-f494-48ac-b554-f271c523bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test statistici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "123a3723-ef7d-4607-bc6c-e1c4cfc86d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Carico GPQA da Hugging Face...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "import pyterrier_rag as ptr\n",
    "from datasets import load_dataset\n",
    "import ir_datasets\n",
    "import pyterrier_alpha as pta\n",
    "from pyterrier_rag.backend import OpenAIBackend\n",
    "from pyterrier_rag.prompt import Concatenator\n",
    "from pyterrier_rag.readers import Reader\n",
    "from pyterrier_rag.prompt import PromptTransformer, prompt\n",
    "from fastchat.model import get_conversation_template\n",
    "import os \n",
    "\n",
    "print(\"🔍 Carico GPQA da Hugging Face...\")\n",
    "mmlu = load_dataset(\"Idavidrein/gpqa\", \"gpqa_diamond\", split=\"train\")\n",
    "df = pd.DataFrame(mmlu)\n",
    "df = df.rename(columns={\"Record ID\": \"qid\",\"Question\": \"query\", \"Correct Answer\": \"gold_answer\"})\n",
    "df = df[[\"qid\", \"query\", \"gold_answer\"]]\n",
    "\n",
    "df_output_base = pd.read_csv(\"../data/processed/rag/base_gpqa_rag_output_cut_5.tsv\", sep=\"\\t\")\n",
    "df_output_ea   = pd.read_csv(\"../data/processed/rag/ea_gpqa_rag_output_cut_5.tsv\",   sep=\"\\t\")\n",
    "\n",
    "df_run_base = pd.read_csv(\"../data/processed/rag/__setencoder-novelty-base__msmarco-segment-gpqa.tsv\", sep=\"\\t\",\n",
    "    names=[\"qid\", \"Q0\", \"docno\", \"rank\", \"score\", \"run_name\", \"text\"]\n",
    ")\n",
    "df_run_ea = pd.read_csv(\"../data/processed/rag/__setencoder-novelty-ea__msmarco-segment-gpqa.tsv\", sep=\"\\t\",\n",
    "    names=[\"qid\", \"Q0\", \"docno\", \"rank\", \"score\", \"run_name\", \"text\"]\n",
    ")\n",
    "\n",
    "dataset = ir_datasets.load('msmarco-segment-v2.1')\n",
    "pt_dataset = pt.get_dataset(\"irds:msmarco-segment-v2.1\")\n",
    "index = pta.Artifact.from_hf('namawho/msmarco-segment-v2.1.pisa')\n",
    "\n",
    "model_name = \"llama-3.3-70b-instruct\"\n",
    "system_message = r\"\"\"You are an expert Q&A system that is trusted around the world. \n",
    "        Always answer the query using the provided context information, and not prior knowledge.\n",
    "\n",
    "        Some rules to follow:\n",
    "        1. Never directly reference the given context in your answer\n",
    "        2. Avoid statements like 'Based on the context, ...' or \n",
    "        'The context information ...' or anything along those lines.\"\"\"\n",
    "prompt_text = \"\"\"Context information is below.\n",
    "            ---------------------\n",
    "            {{ qcontext }}\n",
    "            ---------------------\n",
    "            Given the context information, answer to the given query.\n",
    "            \n",
    "            Query: {{ query }}\n",
    "\n",
    "            Do not include any explanation or additional text.\n",
    "\n",
    "            Answer: \"\"\"\n",
    "\n",
    "template = get_conversation_template(\"meta-llama-3.1-sp\")\n",
    "prompt = PromptTransformer(\n",
    "    conversation_template=template,\n",
    "    system_message=system_message,\n",
    "    instruction=prompt_text,\n",
    "    input_fields=[\"query\", \"qcontext\"],\n",
    "    api_type=\"openai\"\n",
    ")\n",
    "\n",
    "def rename_segment(run):\n",
    "    run = run.rename(columns={\"segment\": \"text\"})\n",
    "    return run\n",
    "rename_pipe = pt.apply.generic(rename_segment)\n",
    "\n",
    "def get_rank_base(df_queries):\n",
    "    run = df_queries.merge(df_run_base, on=\"qid\", how=\"left\")\n",
    "    return run\n",
    "get_rank_base_pipe = pt.apply.generic(get_rank_base)\n",
    "\n",
    "def get_rank_ea(df_queries):\n",
    "    run = df_queries.merge(df_run_ea, on=\"qid\", how=\"left\")\n",
    "    return run\n",
    "get_rank_ea_pipe = pt.apply.generic(get_rank_ea)\n",
    "\n",
    "def get_output_base(df_queries):\n",
    "    print(df_output_base.head(1))\n",
    "    return df_output_base\n",
    "get_output_base_pipe = pt.apply.generic(get_output_base)\n",
    "\n",
    "def get_output_ea(df_queries):\n",
    "    return df_output_ea\n",
    "get_output_ea_pipe = pt.apply.generic(get_output_ea)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"casperhansen/llama-3-8b-instruct-awq\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"casperhansen/llama-3.3-70b-instruct-awq\")\n",
    "\n",
    "generation_args={\n",
    "    \"temperature\": 0.6,\n",
    "    \"max_tokens\": 140,\n",
    "}\n",
    "\n",
    "# this could equally be a real OpenAI models\n",
    "llama = OpenAIBackend(model_name, \n",
    "                      api_key=os.environ['IDA_LLM_API_KEY'],\n",
    "                      generation_args=generation_args,\n",
    "                      base_url=\"http://api.llm.apps.os.dcs.gla.ac.uk/v1\", \n",
    "                      verbose=True, \n",
    "                      parallel=64)\n",
    "llama_reader = Reader(llama, prompt=prompt)\n",
    "\n",
    "pipe_base = get_rank_base_pipe % 5 >>  pt.text.get_text(pt_dataset, \"segment\") >> rename_pipe >> Concatenator(tokenizer=tokenizer, max_length=8191,max_per_context=1638) >> llama_reader\n",
    "pipe_ea = get_rank_ea_pipe % 5 >>  pt.text.get_text(pt_dataset, \"segment\") >> rename_pipe >> Concatenator(tokenizer=tokenizer, max_length=8191,max_per_context=1638) >> llama_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daf63f6a-252a-4a55-99b3-16db4255f5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt.Experiment:   0%|          | 0/16 [00:00<?, ?batches/s]/opt/miniconda3/envs/emalir-dr-rag/lib/python3.10/site-packages/pyterrier/transformer.py:101: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  return pd.DataFrame(list(self.transform_iter(inp.to_dict(orient='records'))))\n",
      "pt.Experiment:   6%|▋         | 1/16 [01:38<24:43, 98.92s/batches]/opt/miniconda3/envs/emalir-dr-rag/lib/python3.10/site-packages/pyterrier/transformer.py:101: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  return pd.DataFrame(list(self.transform_iter(inp.to_dict(orient='records'))))\n",
      "pt.Experiment:  12%|█▎        | 2/16 [03:14<22:38, 97.07s/batches]/opt/miniconda3/envs/emalir-dr-rag/lib/python3.10/site-packages/pyterrier/transformer.py:101: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  return pd.DataFrame(list(self.transform_iter(inp.to_dict(orient='records'))))\n",
      "pt.Experiment:  19%|█▉        | 3/16 [04:57<21:35, 99.66s/batches]/opt/miniconda3/envs/emalir-dr-rag/lib/python3.10/site-packages/pyterrier/transformer.py:101: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  return pd.DataFrame(list(self.transform_iter(inp.to_dict(orient='records'))))\n",
      "pt.Experiment:  25%|██▌       | 4/16 [06:43<20:27, 102.30s/batches]/opt/miniconda3/envs/emalir-dr-rag/lib/python3.10/site-packages/pyterrier/transformer.py:101: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  return pd.DataFrame(list(self.transform_iter(inp.to_dict(orient='records'))))\n",
      "pt.Experiment:  31%|███▏      | 5/16 [08:26<18:47, 102.54s/batches]/opt/miniconda3/envs/emalir-dr-rag/lib/python3.10/site-packages/pyterrier/transformer.py:101: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  return pd.DataFrame(list(self.transform_iter(inp.to_dict(orient='records'))))\n",
      "pt.Experiment:  38%|███▊      | 6/16 [10:12<17:15, 103.54s/batches]/opt/miniconda3/envs/emalir-dr-rag/lib/python3.10/site-packages/pyterrier/transformer.py:101: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  return pd.DataFrame(list(self.transform_iter(inp.to_dict(orient='records'))))\n",
      "pt.Experiment:  44%|████▍     | 7/16 [12:18<16:37, 110.81s/batches]/opt/miniconda3/envs/emalir-dr-rag/lib/python3.10/site-packages/pyterrier/transformer.py:101: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  return pd.DataFrame(list(self.transform_iter(inp.to_dict(orient='records'))))\n",
      "pt.Experiment:  50%|█████     | 8/16 [13:50<14:00, 105.10s/batches]/opt/miniconda3/envs/emalir-dr-rag/lib/python3.10/site-packages/pyterrier/transformer.py:101: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  return pd.DataFrame(list(self.transform_iter(inp.to_dict(orient='records'))))\n",
      "pt.Experiment:  56%|█████▋    | 9/16 [15:32<12:08, 104.11s/batches]/opt/miniconda3/envs/emalir-dr-rag/lib/python3.10/site-packages/pyterrier/transformer.py:101: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  return pd.DataFrame(list(self.transform_iter(inp.to_dict(orient='records'))))\n",
      "pt.Experiment:  62%|██████▎   | 10/16 [17:07<10:07, 101.31s/batches]/opt/miniconda3/envs/emalir-dr-rag/lib/python3.10/site-packages/pyterrier/transformer.py:101: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  return pd.DataFrame(list(self.transform_iter(inp.to_dict(orient='records'))))\n",
      "pt.Experiment:  69%|██████▉   | 11/16 [18:51<08:29, 101.93s/batches]/opt/miniconda3/envs/emalir-dr-rag/lib/python3.10/site-packages/pyterrier/transformer.py:101: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  return pd.DataFrame(list(self.transform_iter(inp.to_dict(orient='records'))))\n",
      "pt.Experiment:  75%|███████▌  | 12/16 [20:41<06:57, 104.49s/batches]/opt/miniconda3/envs/emalir-dr-rag/lib/python3.10/site-packages/pyterrier/transformer.py:101: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  return pd.DataFrame(list(self.transform_iter(inp.to_dict(orient='records'))))\n",
      "pt.Experiment:  81%|████████▏ | 13/16 [22:30<05:17, 105.96s/batches]/opt/miniconda3/envs/emalir-dr-rag/lib/python3.10/site-packages/pyterrier/transformer.py:101: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  return pd.DataFrame(list(self.transform_iter(inp.to_dict(orient='records'))))\n",
      "pt.Experiment:  88%|████████▊ | 14/16 [24:21<03:34, 107.30s/batches]/opt/miniconda3/envs/emalir-dr-rag/lib/python3.10/site-packages/pyterrier/transformer.py:101: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  return pd.DataFrame(list(self.transform_iter(inp.to_dict(orient='records'))))\n",
      "pt.Experiment:  94%|█████████▍| 15/16 [26:26<01:52, 112.73s/batches]/opt/miniconda3/envs/emalir-dr-rag/lib/python3.10/site-packages/pyterrier/transformer.py:101: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  return pd.DataFrame(list(self.transform_iter(inp.to_dict(orient='records'))))\n",
      "pt.Experiment: 100%|██████████| 16/16 [28:01<00:00, 105.11s/batches]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>EM</th>\n",
       "      <th>EM +</th>\n",
       "      <th>EM -</th>\n",
       "      <th>EM p-value</th>\n",
       "      <th>EM reject</th>\n",
       "      <th>EM p-value corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>set encoder base</td>\n",
       "      <td>0.025253</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>set encoder ea</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name        EM  EM +  EM -  EM p-value  EM reject  \\\n",
       "0  set encoder base  0.025253   NaN   NaN         NaN      False   \n",
       "1    set encoder ea  0.030303   1.0   0.0         1.0      False   \n",
       "\n",
       "   EM p-value corrected  \n",
       "0                   NaN  \n",
       "1                   1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "import numpy as np\n",
    "\n",
    "def mcnemar_callable(a, b):\n",
    "    # a,b = array per-query di EM (0/1) dei due modelli, stesso ordine di qid\n",
    "    a = np.asarray(a).astype(int)\n",
    "    b = np.asarray(b).astype(int)\n",
    "    # Discordanti:\n",
    "    only_a = np.sum((a == 1) & (b == 0))\n",
    "    only_b = np.sum((a == 0) & (b == 1))\n",
    "    table = [[0, only_a],\n",
    "             [only_b, 0]]\n",
    "    res = mcnemar(table, exact=False, correction=True)\n",
    "    # PyTerrier si aspetta (stat, pvalue)\n",
    "    return (res.statistic, res.pvalue)\n",
    "\n",
    "pt.Experiment(\n",
    "    [pipe_base, pipe_ea],\n",
    "    df[['qid', 'query']],\n",
    "    df[['qid', 'gold_answer']],\n",
    "    [ptr.measures.EM],\n",
    "    batch_size=25,\n",
    "    verbose=True,\n",
    "    baseline=0, \n",
    "    test=mcnemar_callable, \n",
    "    correction=\"holm\",\n",
    "    names=['set encoder base', 'set encoder ea'],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:emalir-dr-rag]",
   "language": "python",
   "name": "conda-env-emalir-dr-rag-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
