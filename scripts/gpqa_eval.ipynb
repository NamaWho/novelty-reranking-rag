{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cefb03b-ca0a-485c-a93e-ad12bf4a1387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyterrier as pt\n",
    "import pyterrier_rag as ptr\n",
    "from datasets import load_dataset\n",
    "import ir_datasets\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pyterrier_alpha as pta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b9b8566-7a7a-42ee-b1fb-1029d3f53d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Carico GPQA da Hugging Face...\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 2. Caricamento dataset GPQA\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"üîç Carico GPQA da Hugging Face...\")\n",
    "mmlu = load_dataset(\"Idavidrein/gpqa\", \"gpqa_diamond\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c46c13f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a DataFrame from the dataset with 'query' and 'gold_answer' columns\n",
    "df = pd.DataFrame(mmlu)\n",
    "df = df.rename(columns={\"Record ID\": \"qid\",\"Question\": \"query\", \"Correct Answer\": \"gold_answer\"})\n",
    "df = df[[\"qid\", \"query\", \"gold_answer\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b42c29-ddc4-4045-8007-4b2477bc186e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9380cc26-fab2-4345-a142-cef6132ca01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.rename(columns={\"qid\": \"qid\", \"query\": \"text\"})\n",
    "data = data[['qid', 'text']]\n",
    "data[\"text\"] = data[\"text\"].str.replace('\\n', '\\\\n')\n",
    "data.to_csv(\"../data/raw/rag/gpqa-queries.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82d6cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ir_datasets.load('msmarco-segment-v2.1')\n",
    "pt_dataset = pt.get_dataset(\"irds:msmarco-segment-v2.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b80435f6-7e85-4517-9f96-8694c46b5f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_segment(run):\n",
    "    run = run.rename(columns={\"segment\": \"text\"})\n",
    "    return run\n",
    "rename_pipe = pt.apply.generic(rename_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "859aa54b-1291-4326-b11f-d57d1d99cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pta.Artifact.from_hf('namawho/msmarco-segment-v2.1.pisa')\n",
    "bm25_ret = index.bm25(verbose=True) % 200 >> pt.text.get_text(pt_dataset, \"segment\") >> rename_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3abf3013-0033-457f-82d5-39d90ab63e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PISA bm25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [07:17<00:00,  2.21s/query]\n"
     ]
    }
   ],
   "source": [
    "retrieved = bm25_ret.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f26b994-0674-48d0-891d-62cf9230606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved[\"Q0\"] = \"Q0\"\n",
    "retrieved[\"tag\"] = \"bm25\"\n",
    "retrieved = retrieved[[\"qid\",\"Q0\",\"docno\",\"rank\",\"score\",\"tag\"]]\n",
    "\n",
    "# scrivi su file\n",
    "retrieved.to_csv(\"../data/raw/rag/__bm25__msmarco-segment-gpqa.run\", sep=\" \", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b124e450-aabd-4592-881b-95d6617c75f0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eab8258f-3073-4ee8-a8c6-fc3507f8d3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Carico ranking esistente...\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Carico ranking esistente...\")\n",
    "df_run_base = pd.read_csv(\n",
    "    \"../data/processed/rag/__setencoder-novelty-base__msmarco-segment-gpqa.tsv\", sep=\"\\t\",\n",
    "    names=[\"qid\", \"Q0\", \"doc_id\", \"rank\", \"score\", \"run_name\", \"text\"]\n",
    ")\n",
    "df_run_ea = pd.read_csv(\n",
    "    \"../data/processed/rag/__setencoder-novelty-ea__msmarco-segment-gpqa.tsv\", sep=\"\\t\",\n",
    "    names=[\"qid\", \"Q0\", \"doc_id\", \"rank\", \"score\", \"run_name\", \"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9f097ec-5fab-4db1-aa00-c3b8b803adaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 113520750/113520750 [50:15<00:00, 37645.17it/s] \n"
     ]
    }
   ],
   "source": [
    "dataset = ir_datasets.load('msmarco-segment-v2.1')\n",
    "pt_dataset = pt.get_dataset(\"irds:msmarco-segment-v2.1\")\n",
    "total_docs = dataset.docs_count() \n",
    "all_docs = list(tqdm(dataset.docs_iter(), total=total_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5539f99c-d015-4e93-ba48-e45392827e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict = {doc.doc_id: doc for doc in all_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad24324e-d41a-4dd5-8330-76ded4e6d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier_rag.backend import OpenAIBackend\n",
    "from pyterrier_rag.prompt import Concatenator\n",
    "from pyterrier_rag.readers import Reader\n",
    "from pyterrier_rag.prompt import PromptTransformer, prompt\n",
    "from fastchat.model import get_conversation_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1857ec0-22cb-4085-9f6e-eb74c21ec5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"llama-3-8b-instruct\"\n",
    "model_name = \"llama-3.3-70b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae4aad4c-346c-40af-a0c3-4a4a0b076073",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = r\"\"\"You are an expert Q&A system that is trusted around the world. \n",
    "        Always answer the query using the provided context information, and not prior knowledge.\n",
    "\n",
    "        Some rules to follow:\n",
    "        1. Never directly reference the given context in your answer\n",
    "        2. Avoid statements like 'Based on the context, ...' or \n",
    "        'The context information ...' or anything along those lines.\"\"\"\n",
    "prompt_text = \"\"\"Context information is below.\n",
    "            ---------------------\n",
    "            {{ qcontext }}\n",
    "            ---------------------\n",
    "            Given the context information, answer to the given query.\n",
    "            \n",
    "            Query: {{ query }}\n",
    "\n",
    "            Do not include any explanation or additional text.\n",
    "\n",
    "            Answer: \"\"\"\n",
    "\n",
    "template = get_conversation_template(\"meta-llama-3.1-sp\")\n",
    "prompt = PromptTransformer(\n",
    "    conversation_template=template,\n",
    "    system_message=system_message,\n",
    "    instruction=prompt_text,\n",
    "    input_fields=[\"query\", \"qcontext\"],\n",
    "    api_type=\"openai\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c8cfc4c-c402-4586-949f-7e3d56e5f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank(df_queries):\n",
    "    run = df_queries.merge(df_run_ea, on=\"qid\", how=\"left\")\n",
    "    return run\n",
    "get_rank_pipe = pt.apply.generic(get_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78513c1f-8861-459e-bb9c-5c96f1fb78fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"casperhansen/llama-3-8b-instruct-awq\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"casperhansen/llama-3.3-70b-instruct-awq\")\n",
    "\n",
    "generation_args={\n",
    "    \"temperature\": 0.6,\n",
    "    \"max_tokens\": 140,\n",
    "}\n",
    "\n",
    "# this could equally be a real OpenAI models\n",
    "llama = OpenAIBackend(model_name, \n",
    "                      api_key=os.environ['IDA_LLM_API_KEY'],\n",
    "                      generation_args=generation_args,\n",
    "                      base_url=\"http://api.llm.apps.os.dcs.gla.ac.uk/v1\", \n",
    "                      verbose=True, \n",
    "                      parallel=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6058c64-f79a-46f5-8f12-08dcec909ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_reader = Reader(llama, prompt=prompt)\n",
    "set_encoder_llama = get_rank_pipe % 5 >> Concatenator(tokenizer=tokenizer, max_length=8191,max_per_context=1638) >> llama_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba5dd0-dec5-4844-b2a7-428f2f4489f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® Nessun salvataggio trovato, si parte da zero.\n",
      "üß† Da processare: 198 esempi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÅ RAG on MMLU:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 100/198 [03:54<01:55,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Salvati 100 risultati su ../data/processed/rag/ea_gpqa_rag_output_cut_5.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÅ RAG on MMLU:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 166/198 [07:10<02:14,  4.20s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "save_every=100\n",
    "partial_save_path=\"../data/processed/rag/ea_gpqa_rag_output_cut_5.tsv\"\n",
    "\n",
    "try:\n",
    "    df_partial = pd.read_csv(partial_save_path, sep=\"\\t\")\n",
    "    done_qids = set(df_partial[\"qid\"])\n",
    "    print(f\"‚úÖ Ripresi {len(done_qids)} risultati da salvataggio parziale.\")\n",
    "    results = [df_partial]\n",
    "except FileNotFoundError:\n",
    "    print(\"üö® Nessun salvataggio trovato, si parte da zero.\")\n",
    "    done_qids = set()\n",
    "    results = []\n",
    "\n",
    "remaining = df[~df[\"qid\"].isin(done_qids)].reset_index(drop=True)\n",
    "print(f\"üß† Da processare: {len(remaining)} esempi.\")\n",
    "    \n",
    "for row in tqdm(remaining.iterrows(), total=len(remaining), desc=\"üîÅ RAG on MMLU\"):\n",
    "    idx, data = row\n",
    "    result = set_encoder_llama.transform(pd.DataFrame([data]))\n",
    "\n",
    "    result_merged = result.merge(df[[\"qid\", \"gold_answer\"]], on=\"qid\", how=\"left\")\n",
    "    results.append(result_merged)\n",
    "\n",
    "    # ‚úÖ Salvataggio intermedio\n",
    "    if (idx + 1) % save_every == 0 or (idx + 1) == len(remaining):\n",
    "        df_save = pd.concat(results, ignore_index=True)\n",
    "        df_save.to_csv(partial_save_path, sep=\"\\t\", index=False)\n",
    "        print(f\"üíæ Salvati {len(df_save)} risultati su {partial_save_path}\")\n",
    "        results = [df_save]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95f55baa-6a66-4113-9678-d53d664b8582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt.Experiment: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [05:35<00:00, 41.99s/batches]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>F1</th>\n",
       "      <th>EM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>set encoder</td>\n",
       "      <td>0.127162</td>\n",
       "      <td>0.035354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name        F1        EM\n",
       "0  set encoder  0.127162  0.035354"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.Experiment(\n",
    "    [set_encoder_llama],\n",
    "    df[['qid', 'query']], # NB: remove .head() to run on all dev topics\n",
    "    df[['qid', 'gold_answer']],\n",
    "    [ptr.measures.F1, ptr.measures.EM],\n",
    "    batch_size=25,\n",
    "    verbose=True,\n",
    "    names=['set encoder'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0122d3-7711-4521-910e-ec41542c8cfb",
   "metadata": {},
   "source": [
    "# Score@3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d732b61-9df5-42ec-bfbc-a20f02d3d965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name\tF1\tEM\n",
    "# set encoder base\t0.134688\t0.040404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec8b58-3c3b-47ad-99a9-5ac934e924e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name\tF1\tEM\n",
    "# 0\tset encoder ea\t0.133429\t0.040404"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566e4471-bc6d-478a-8c30-a3770160d39a",
   "metadata": {},
   "source": [
    "# Score@5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c7f47-42da-4092-ae9d-eca702576402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \tname\tF1\tEM\n",
    "#   set encoder\tbase 0.128475\t0.035354\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aef7ea0-e43e-44ed-a7cb-dba5eaea7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name\tF1\tEM\n",
    "# 0\tset encoder ea\t0.127162\t0.035354"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ptrag]",
   "language": "python",
   "name": "conda-env-ptrag-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
